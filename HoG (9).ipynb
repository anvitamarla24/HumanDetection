{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.img = np.full((160, 96), 0)\n",
    "        self.img_array = np.full((160, 96), 0)\n",
    "        self.gray = np.full((160, 96), 0)\n",
    "        self.gray_array = np.full((160, 96), 0)\n",
    "        \n",
    "        self.G_X = np.full((160, 96), 0)\n",
    "        self.G_Y = np.full((160, 96), 0)\n",
    "        self.g_x = np.full((160, 96), 0)\n",
    "        self.g_y = np.full((160, 96), 0)\n",
    "        self.g_mag = np.full((160, 96), 0)\n",
    "        self.sobel_image = np.full((160, 96), 0)\n",
    "        \n",
    "        self.theta = np.full((160, 96), 0)       \n",
    "        \n",
    "        self.path = path\n",
    "        self.img_array = np.full((160, 96), 0)\n",
    "        self.image_read()\n",
    "        self.gray = self.rgb2gray(self.img_array)\n",
    "        self.image_write(self.gray)\n",
    "        \n",
    "        self.g_x, self.g_y, self.sobel_output = np.round(self.sobels_operator(self.gray))\n",
    "        self.theta = self.gradient_angles(self.gray_array, self.g_x, self.g_y)\n",
    "#         print(self.g_x[28][28])\n",
    "#         print(self.g_y[28][28])\n",
    "#         print(self.theta[28][28])\n",
    "        \n",
    "#         return  self.img_array, self.gray_array, self.g_mag, self.theta\n",
    "        \n",
    "#         self\n",
    "    \n",
    "    def image_read(self):\n",
    "        self.img = cv2.imread(self.path)  \n",
    "        self.img_array = np.array(self.img, dtype=float)\n",
    "#         self.gray = self.rgb2gray(self.img)\n",
    "#         self.\n",
    "    \n",
    "    def rgb2gray(self, rgb):\n",
    "        return np.round(np.dot(rgb[...,:3], [0.299, 0.587, 0.114]))\n",
    "    \n",
    "    def image_write(self, gray):\n",
    "        self.img_gray = cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray.bmp', self.gray)\n",
    "        self.gray_array = np.array(gray, dtype=float)\n",
    "#         return self.gray_array\n",
    "\n",
    "    #sobels operator\n",
    "    def sobels_operator(self, inp):\n",
    "        self.inp = inp\n",
    "        g_x = np.array([[-1,0,1],\n",
    "                       [-2,0,2],\n",
    "                       [-1,0,1]])\n",
    "        g_y = np.array([[1,2,1],\n",
    "                       [0,0,0],\n",
    "                       [-1,-2,-1]])\n",
    "    \n",
    "        #Horizontal Gradient \n",
    "        self.G_X = self.conv_sobel(self.inp,g_x)\n",
    "        #Saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobelGX.bmp', self.G_X)\n",
    "       \n",
    "        #Vertical Gradient\n",
    "        self.G_Y = self.conv_sobel(self.inp,g_y)\n",
    "        #saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobelGY.bmp', self.G_Y)\n",
    "    \n",
    "        #Gradient Magnitude\n",
    "        self.g_mag = np.sqrt(np.square(self.G_X) + np.square(self.G_Y))\n",
    "        #saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobel.bmp', self.g_mag)    \n",
    "        return self.G_X, self.G_Y, self.g_mag\n",
    "\n",
    "    #convolution done by implementing using 2D directly\n",
    "    def conv_sobel(self,x,y):\n",
    "        \n",
    "        row_size = x.shape[0]\n",
    "        column_size = x.shape[1]        \n",
    "        conv_img = np.zeros((row_size, column_size), dtype=np.float)\n",
    "        \n",
    "        for a in range(0, row_size - 2):\n",
    "            for b in range(0, column_size - 2):\n",
    "                \n",
    "                conv_img[a + 1][b + 1] = (np.sum(x[a: a + 3, b: b + 3] * y)) / 4\n",
    "\n",
    "        return conv_img\n",
    "    \n",
    "    def gradient_angles(self, gray_array, g_x, g_y):\n",
    "        #Finding the Gradient Angles i.e., theta\n",
    "        print(self.gray_array.shape[0])\n",
    "        #arctan2 is tan inverse, which gives us results in radians with [-pi to pi]\n",
    "        self.theta = np.zeros((self.gray_array.shape[0],self.gray_array.shape[1]))\n",
    "        for i in range(self.gray_array.shape[0]):\n",
    "            for j in range(0, self.gray_array.shape[1]):\n",
    "                if(self.g_y[i][j] == 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = 0\n",
    "                elif(self.g_y[i][j] > 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = 90\n",
    "                elif(self.g_y[i][j] < 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = -90\n",
    "                elif(self.g_y[i][j] == 0):\n",
    "                    self.theta[i][j] = 0\n",
    "                else:\n",
    "                    if(self.g_x[i][j] == 0):\n",
    "                        print('here')\n",
    "                    self.theta[i][j] = np.arctan2(self.g_y[i][j],self.g_x[i][j])\n",
    "    \n",
    "        #Converting radians to degrees\n",
    "        self.theta = np.rad2deg(self.theta)\n",
    "        #converting all the negatives into positives by adding 360 so the range is now [0 to 360]\n",
    "        for i in range(0, self.gray_array.shape[0]):\n",
    "            for j in range(0, self.gray_array.shape[1]):\n",
    "                if(self.theta[i][j] < 0):\n",
    "                    self.theta[i][j] += 360\n",
    "                    \n",
    "        return self.theta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram:\n",
    "    def __init__(self, mag_block, theta_block):\n",
    "        self.center = np.array([0,20,40,60,80,100,120,140,160])\n",
    "#         4 histograms of 9 bins each - per block\n",
    "        self.bins = np.zeros((4,9), dtype=float)\n",
    "        self.cell_size = 8\n",
    "        self.cells = []\n",
    "        self.t_cells = []\n",
    "        self.m_cells = []\n",
    "        self.flattened = []\n",
    "        self.theta_block = theta_block\n",
    "        self.mag_block = mag_block\n",
    "        self.convert_block_2_cell(mag_block,theta_block)\n",
    "#         c=0\n",
    "#         print(c)\n",
    "        self.generate_features(self.m_cells, self.t_cells)\n",
    "        self.norm_hog()\n",
    "        self.flattened_bins()\n",
    "        \n",
    "    def flattened_bins(self):\n",
    "        for bin in self.bins:\n",
    "            for item in bin:\n",
    "                self.flattened.append(item)\n",
    "        \n",
    "    \n",
    "    def norm_hog(self):\n",
    "        \n",
    "        summation = 0\n",
    "        for bin in self.bins:\n",
    "            for j in range(len(bin)):\n",
    "                summation += bin[j] ** 2\n",
    "        \n",
    "        dist = summation ** 0.5\n",
    "                   \n",
    "        for bin in self.bins:\n",
    "            for j in range(len(bin)):\n",
    "                if(dist == 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    bin[j] /= dist\n",
    "\n",
    "    \n",
    "    def convert_block_2_cell(self, mag_block, theta_block):\n",
    "        c=0\n",
    "        for m in range(0,theta_block.shape[0], self.cell_size):\n",
    "                for n in range(0, theta_block.shape[1], self.cell_size):\n",
    "                    \n",
    "                    m_cell = np.zeros((self.cell_size,self.cell_size))\n",
    "                    t_cell = np.zeros((self.cell_size,self.cell_size))\n",
    "                    \n",
    "                    for p in range(self.cell_size):\n",
    "                        for q in range(self.cell_size):\n",
    "                            \n",
    "                            m_cell[p][q] = mag_block[m+p][n+q]\n",
    "                            t_cell[p][q] = theta_block[m+p][n+q]\n",
    "                     \n",
    "                    c+=1\n",
    "                    self.m_cells.append(m_cell)\n",
    "                    self.t_cells.append(t_cell)\n",
    "#         print(c)\n",
    "    \n",
    "    def generate_features(self, m_cells, t_cells):\n",
    "        \n",
    "        for i in range(len(t_cells)):\n",
    "            for j in range(len(t_cells[i])):\n",
    "                for k in range(len(t_cells[i][j])):\n",
    "                    self.cal_hist(m_cells[i][j][k],t_cells[i][j][k], self.bins[i]) \n",
    "                \n",
    "# HANDLE EDGE CASE : -20,10       \n",
    "    def cal_hist(self, mag, angle, bins):       \n",
    "        \n",
    "#         COMPUTE THE DISTANCE TO ADD THE MAG WRT THE CENTER\n",
    "        if(angle >= 180):\n",
    "            angle -= 180\n",
    "#         now we have all unsigned angles\n",
    "# new case to consider: if angle is >=160 first = bin 9 and second is bin 1\n",
    "        \n",
    "        if(angle >= 160):\n",
    "            first_bin = 8\n",
    "            second_bin = 0\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag \n",
    "#         edge care , if angle is >=350 it's between bins 1 and 9\n",
    "        \n",
    "        if(angle >= 0 and angle < 20):\n",
    "            first_bin = 0\n",
    "            second_bin = 1\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 20 and angle < 40):\n",
    "            first_bin = 1\n",
    "            second_bin = 2\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "        if(angle >= 40 and angle < 60):\n",
    "            first_bin = 2\n",
    "            second_bin = 3\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "        if(angle >= 60 and angle < 80):\n",
    "            first_bin = 3\n",
    "            second_bin = 4\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 80 and angle < 100):\n",
    "            first_bin = 4\n",
    "            second_bin = 5\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 100 and angle < 120):\n",
    "            first_bin = 5\n",
    "            second_bin = 6\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 120 and angle < 140):\n",
    "            first_bin = 6\n",
    "            second_bin = 7\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "         \n",
    "        if(angle >= 140 and angle < 160):\n",
    "            first_bin = 7\n",
    "            second_bin = 8\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "    def calc_distance(self, angle, center):        \n",
    "        percent = (np.absolute(angle - center))/20\n",
    "        return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_feature(mag, theta): \n",
    "    descriptor = []\n",
    "    cell_size = 8\n",
    "    block_size = 16\n",
    "    block_overlap = 8\n",
    "    assert(mag.shape[0] == theta.shape[0])\n",
    "    assert(mag.shape[1] == theta.shape[1])\n",
    "    b=0\n",
    "    c=0\n",
    "    \n",
    "#     generating blocks from pixels\n",
    "    for i in range(0,theta.shape[0]-cell_size, cell_size):\n",
    "        for j in range(0,theta.shape[1]-cell_size, cell_size):\n",
    "            \n",
    "            m_block = np.zeros((block_size,block_size))\n",
    "            t_block = np.zeros((block_size,block_size))\n",
    "            \n",
    "            for k in range(block_size):\n",
    "                for l in range(block_size):  \n",
    "                    \n",
    "                    m_block[k][l] = mag[i+k][j+l]\n",
    "                    t_block[k][l] = theta[i+k][j+l]\n",
    "            b +=1\n",
    "\n",
    "            hist_obj = Histogram(m_block,t_block)\n",
    "            for item in hist_obj.flattened:\n",
    "                descriptor.append(item)\n",
    "    print(b)\n",
    "#     print(c)\n",
    "    return descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_feature_output = np.array(h_feature(sobel_output,theta))\n",
    "# print(np.concatenate(h_feature_output).ravel().tolist())\n",
    "# print(h_feature_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBP:\n",
    "    def __init__(self, gray_block):\n",
    "        self.mag = gray_block\n",
    "        self.bins = np.zeros((59,), dtype=float)\n",
    "        self.generate_features(gray_block)\n",
    "        self.norm_lbp()\n",
    "        \n",
    "    def norm_lbp(self):\n",
    "        for i in range(len(self.bins)):\n",
    "            self.bins[i] = self.bins[i] / 256\n",
    "    \n",
    "    def generate_features(self,gray_block):\n",
    "        for i in range(gray_block.shape[0]):\n",
    "            for j in range(gray_block.shape[1]):\n",
    "                self.compute_LBP_pattern(i,j,gray_block)\n",
    "        \n",
    "#     def norm_lbp(self):\n",
    "#         for i in range(len(self.bins)):\n",
    "#             self.bins[i] = self.bins[i] / 256\n",
    "        \n",
    "    def compute_LBP_pattern(self,i, j,gray_block):\n",
    "        x = gray_block\n",
    "        pattern = ''\n",
    "        \n",
    "        try:\n",
    "            if(x[i-1][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i-1][j] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i-1][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i+1][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i+1][j] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i+1][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            self.append_to_bins(pattern)\n",
    "            \n",
    "        except IndexError:\n",
    "            pattern = '00000101'\n",
    "            self.append_to_bins(pattern)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def append_to_bins(self, lbp_pattern):        \n",
    "        decimal = int(lbp_pattern, 2)\n",
    "        patterns = {\n",
    "            0: 0,\n",
    "            1: 1,\n",
    "            2: 2,\n",
    "            3: 3,\n",
    "            4: 4,\n",
    "            6: 5,\n",
    "            7: 6,\n",
    "            8: 7,\n",
    "            12: 8,\n",
    "            14: 9,\n",
    "            15: 10,\n",
    "            16: 11,\n",
    "            24: 12,\n",
    "            28: 13,\n",
    "            30: 14,\n",
    "            31: 15,\n",
    "            32: 16,\n",
    "            48: 17,\n",
    "            56: 18,\n",
    "            60: 19,\n",
    "            62: 20,\n",
    "            63: 21,\n",
    "            64: 22,\n",
    "            96: 23,\n",
    "            112: 24,\n",
    "            120: 25,\n",
    "            124: 26,\n",
    "            126: 27,\n",
    "            127: 28,\n",
    "            128: 29,\n",
    "            129: 30,\n",
    "            131: 31,\n",
    "            135: 32,\n",
    "            143: 33,\n",
    "            159: 34,\n",
    "            191: 35,\n",
    "            192: 36,\n",
    "            193: 37,\n",
    "            195: 38,\n",
    "            199: 39,\n",
    "            207: 40,\n",
    "            223: 41,\n",
    "            224: 42,\n",
    "            225: 43,\n",
    "            227: 44,\n",
    "            231: 45,\n",
    "            239: 46,\n",
    "            240: 47,\n",
    "            241: 48,\n",
    "            243: 49,\n",
    "            247: 50,\n",
    "            248: 51,\n",
    "            249: 52,\n",
    "            251: 53,\n",
    "            252: 54,\n",
    "            253: 55,\n",
    "            254: 56,\n",
    "            255: 57\n",
    "        }\n",
    "        bin_number = patterns.get(decimal, 58)\n",
    "        self.bins[bin_number] += 1   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LBP_feature(gray):\n",
    "    lbp_descriptor = []\n",
    "    cell_size = 8\n",
    "    block_size = 16\n",
    "    block_overlap = 8\n",
    "    b=0\n",
    "#     c=0\n",
    "    \n",
    "#     generating blocks from pixels   \n",
    "    for i in range(0,gray.shape[0],block_size):\n",
    "        for j in range(0,gray.shape[1],block_size):\n",
    "            \n",
    "            gray_block = np.zeros((block_size,block_size))\n",
    "            \n",
    "            for p in range(block_size):\n",
    "                for q in range(block_size):\n",
    "                    \n",
    "                    gray_block[p][q] = gray[i+p][j+q]\n",
    "            \n",
    "            \n",
    "            b+=1\n",
    "            lbp_obj1 = LBP(gray_block)\n",
    "            lbp_descriptor.append(lbp_obj1.bins)\n",
    "#     print(b)        \n",
    "    return lbp_descriptor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbp_output = LBP_feature(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(lbp_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOG_NeuralNetwork:\n",
    "    def __init__(self, human_feature_vectors, non_human_feature_vectors, epochs = 3, alpha = 0.01, input_layer_neurons = 7524, hidden_layer_neurons = 200, output_layer_neurons = 1):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.input_layer_neurons = input_layer_neurons\n",
    "        self.hidden_layer_neurons = hidden_layer_neurons\n",
    "        self.output_layer_neurons = output_layer_neurons\n",
    "        \n",
    "#         self.output_hidden_layer = np.full((2, 1), 0.0)\n",
    "#         self.pred_output = np.full((, 1), 0.0)\n",
    "#         self.err = np.full((2, 1), 0.0)\n",
    "        \n",
    "        #assigning random weights\n",
    "#         self.hidden_layer_weights = np.random.randn(-0.5, 0.5, size = (self.input_layer_neurons,self.hidden_layer_neurons))\n",
    "#         self.output_layer_weights = np.random.randn(-0.5, 0.5, size = (self.hidden_layer_neurons,self.output_layer_neurons))\n",
    "        self.hidden_layer_weights =  (np.random.random_sample((self.input_layer_neurons,self.hidden_layer_neurons)) -0.5) * 2 * self.alpha\n",
    "        self.output_layer_weights =  (np.random.random_sample((self.hidden_layer_neurons,self.output_layer_neurons)) - 0.5) * 2 * self.alpha\n",
    "    \n",
    "        #assigning random bias       \n",
    "        self.hidden_layer_bias = np.full((1,self.hidden_layer_neurons), -1.0, dtype = float)\n",
    "        self.output_layer_bias = np.full((1,self.output_layer_neurons), -1.0, dtype = float)\n",
    "        \n",
    "#         self.human_feature_vectors = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "        self.human_feature_vectors = human_feature_vectors\n",
    "        self.non_human_feature_vectors = non_human_feature_vectors\n",
    "        \n",
    "        self.output_hidden_layer = None\n",
    "        self.pred_output = None\n",
    "        self.pred_output_delta = None\n",
    "        self.hidden_layer_output_delta = None\n",
    "        \n",
    "        \n",
    "        print(self.non_human_feature_vectors.shape)\n",
    "        self.train_nn()\n",
    "        \n",
    "    # Sigmoid function for the output neuron \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "        \n",
    "    # Derivative of Sigmoid in terms of itself:\n",
    "    def derivative_of_sigmoid(self, x):\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    # ReLu function for the hidden neurons\n",
    "    def relu(self, x):\n",
    "        y = x.copy()\n",
    "        y[x<0] = 0\n",
    "        return y\n",
    "        \n",
    "    # Derivative of ReLU in terms of itself is:    \n",
    "    def derivative_of_relu(self, x):\n",
    "        y = x.copy()\n",
    "        y[x>0] = 1\n",
    "        y[x<=0] = 0\n",
    "        return y\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "#         h_layer_a = np.dot(self.inputs,self.hidden_layer_weights) + self.hidden_layer_bias\n",
    "        self.output_hidden_layer = self.relu(np.dot(self.inputs,self.hidden_layer_weights) + self.hidden_layer_bias)\n",
    "\n",
    "#         o_layer_a = np.dot(self.output_hidden_layer, self.output_layer_weights) + self.output_layer_bias\n",
    "        \n",
    "        self.pred_output = self.sigmoid(np.dot(self.output_hidden_layer, self.output_layer_weights) + self.output_layer_bias)\n",
    "        \n",
    "    def backpropogation(self, target_output):\n",
    "        \n",
    "        final_error = target_output - self.pred_output\n",
    "#         print('final error', final_error)\n",
    "        self.err = np.absolute(target_output - self.pred_output).sum()\n",
    "        self.pred_output_delta = final_error * self.derivative_of_sigmoid(self.pred_output)\n",
    "        \n",
    "        h_layer_error = self.pred_output_delta.dot(self.output_layer_weights.T)\n",
    "        self.hidden_layer_output_delta = h_layer_error * self.derivative_of_relu(self.output_hidden_layer)\n",
    "            \n",
    "    def update(self, inputs):\n",
    "            \n",
    "        #Updating weights and bias\n",
    "#         self.inputs = inputs\n",
    "        self.output_layer_weights = self.output_layer_weights + self.output_hidden_layer.T.dot(self.pred_output_delta) * self.alpha\n",
    "        self.output_layer_bias = self.output_layer_bias + np.sum(self.pred_output_delta,axis = 1, keepdims=True) * self.alpha\n",
    "    \n",
    "        self.hidden_layer_weights = self.hidden_layer_weights + inputs.T.dot(self.hidden_layer_output_delta) * self.alpha\n",
    "        self.hidden_layer_bias = self.hidden_layer_bias + np.sum(self.hidden_layer_output_delta, axis = 1, keepdims=True) * self.alpha\n",
    "\n",
    "        \n",
    "    def train_nn(self):\n",
    "        \n",
    "        avg_error = 0\n",
    "        for _ in range(self.epochs):\n",
    "            target_output = np.array([[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0]]) \n",
    "            self.feedforward(self.human_feature_vectors)\n",
    "            self.backpropogation(target_output)\n",
    "            self.update(self.human_feature_vectors)\n",
    "#             if(np.absolute(avg_error - self.err)<= 0.1):\n",
    "#                 break\n",
    "            avg_error += self.err\n",
    "#             if(avg_error <= 0.1):\n",
    "#                 break\n",
    "        print('Predicted output after training Negative Images:\\n', self.pred_output)\n",
    "        print('Average Error:\\n', avg_error/len(self.human_feature_vectors))\n",
    "        \n",
    "        #save anything from training positive here         \n",
    "        \n",
    "        # reset values for negative features\n",
    "\n",
    "#         #assigning random weights\n",
    "#         self.hidden_layer_weights = np.random.uniform(-0.5, 0.5, size = (self.input_layer_neurons,self.hidden_layer_neurons))\n",
    "#         self.output_layer_weights = np.random.uniform(-0.5, 0.5, size = (self.hidden_layer_neurons,self.output_layer_neurons))\n",
    "\n",
    "#         #assigning random bias       \n",
    "#         self.hidden_layer_bias = np.random.uniform(-1, -1, size = (1,self.hidden_layer_neurons))\n",
    "#         self.output_layer_bias = np.random.uniform(-1, -1, size = (1,self.output_layer_neurons))\n",
    "\n",
    "#         self.pred_output = 0\n",
    "        \n",
    "# #         self.pred_output = np.full((2, 1), 0.0)\n",
    "        \n",
    "#         avg_error = 0\n",
    "#         for _ in range(self.epochs):\n",
    "#             target_output = np.full((10,1),0.0)\n",
    "#             self.feedforward(self.non_human_feature_vectors)\n",
    "#             self.backpropogation(target_output)\n",
    "#             self.update(self.non_human_feature_vectors)\n",
    "#             avg_error += self.err\n",
    "        \n",
    "#         print('Predicted output after training Negative Images:\\n', self.pred_output)\n",
    "#         print('Average Error:\\n', avg_error/len(self.non_human_feature_vectors))\n",
    "        \n",
    "# #         save anything from training negative here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(h_feature_output):\n",
    "#     nn_obj1 = NeuralNetwork()\n",
    "#     flattened_vec = nn_obj1.flat_my_vector(h_feature_output)\n",
    "#     nn_obj1.train_nn(flattened_vec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_train_pos = train(h_feature_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive_hog_feature_vectors = [h_feature_output, h_feature_output]\n",
    "# nn_train_pos = NeuralNetwork(positive_hog_feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nn(X,Y, hidden_layer_neurons, epochs = 100):\n",
    "#     # Step 1: Initialize the weights and biases to have random values between âˆ’0.5 and 0.5:\n",
    "#     # weights\n",
    "#     hidden_layer_weights = np.random.uniform(0,1, size = (input_layer_neurons,hidden_layer_neurons))\n",
    "#     output_layer_weights = np.random.uniform(0,1, size = (hidden_layer_neurons,output_layer_neurons))\n",
    "\n",
    "#     # bias:\n",
    "#     hidden_layer_bias = np.random.uniform(0.0, 0.0,size = (1,hidden_layer_neurons))\n",
    "#     output_layer_bias = np.random.uniform(0.0,0.0,size = (1,output_layer_neurons))\n",
    "\n",
    "#     print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "#     print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "#     print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "#     print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop001008b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[0.19595095 0.00138505 0.00083427 ... 0.00138123 0.00100435 0.0013774 ]]\n",
      "crop001028a.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[0.19595095 0.00138505 0.00083427 ... 0.00138123 0.00100435 0.0013774 ]\n",
      " [0.16378476 0.         0.0009838  ... 0.         0.         0.        ]]\n",
      "crop001030c.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[1.95950949e-01 1.38505293e-03 8.34270939e-04 ... 1.38122701e-03\n",
      "  1.00434612e-03 1.37740236e-03]\n",
      " [1.63784764e-01 0.00000000e+00 9.83801845e-04 ... 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [6.39834643e-01 4.80995038e-04 1.81966218e-02 ... 4.07708082e-03\n",
      "  5.81930438e-03 5.25503311e-03]]\n",
      "crop001045b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " [ 1.05489310e-01  3.53852685e-02  5.42115415e-03 ...  1.06110231e-03\n",
      "   4.26998561e-04 -3.90319288e-02]]\n",
      "crop001047b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " [ 1.05489310e-01  3.53852685e-02  5.42115415e-03 ...  1.06110231e-03\n",
      "   4.26998561e-04 -3.90319288e-02]\n",
      " [ 2.15683061e-01  1.40373603e-02  4.63218939e-04 ...  2.33131357e-03\n",
      "   6.72713881e-03 -6.00040907e-01]]\n",
      "crop001063b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " [ 1.05489310e-01  3.53852685e-02  5.42115415e-03 ...  1.06110231e-03\n",
      "   4.26998561e-04 -3.90319288e-02]\n",
      " [ 2.15683061e-01  1.40373603e-02  4.63218939e-04 ...  2.33131357e-03\n",
      "   6.72713881e-03 -6.00040907e-01]\n",
      " [ 4.54651708e-01  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00 -6.51827204e-02]]\n",
      "crop001275b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " ...\n",
      " [ 2.15683061e-01  1.40373603e-02  4.63218939e-04 ...  2.33131357e-03\n",
      "   6.72713881e-03 -6.00040907e-01]\n",
      " [ 4.54651708e-01  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00 -6.51827204e-02]\n",
      " [ 6.00584169e-01  3.19877589e-02  2.95571683e-02 ...  7.30724209e-03\n",
      "   9.76967060e-03 -3.42552684e-01]]\n",
      "crop001672b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " ...\n",
      " [ 4.54651708e-01  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00 -6.51827204e-02]\n",
      " [ 6.00584169e-01  3.19877589e-02  2.95571683e-02 ...  7.30724209e-03\n",
      "   9.76967060e-03 -3.42552684e-01]\n",
      " [ 0.00000000e+00  2.66096794e-03  8.66209526e-03 ...  3.35679754e-03\n",
      "   4.29142673e-03 -9.68278918e-02]]\n",
      "crop_000010b.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " ...\n",
      " [ 6.00584169e-01  3.19877589e-02  2.95571683e-02 ...  7.30724209e-03\n",
      "   9.76967060e-03 -3.42552684e-01]\n",
      " [ 0.00000000e+00  2.66096794e-03  8.66209526e-03 ...  3.35679754e-03\n",
      "   4.29142673e-03 -9.68278918e-02]\n",
      " [ 5.77067003e-03  7.23693973e-04  4.81953246e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n",
      "person_and_bike_026a.bmp\n",
      "160\n",
      "209\n",
      "(7524,)\n",
      "[[ 1.95950949e-01  1.38505293e-03  8.34270939e-04 ...  1.38122701e-03\n",
      "   1.00434612e-03  1.37740236e-03]\n",
      " [ 1.63784764e-01  0.00000000e+00  9.83801845e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.39834643e-01  4.80995038e-04  1.81966218e-02 ...  4.07708082e-03\n",
      "   5.81930438e-03  5.25503311e-03]\n",
      " ...\n",
      " [ 0.00000000e+00  2.66096794e-03  8.66209526e-03 ...  3.35679754e-03\n",
      "   4.29142673e-03 -9.68278918e-02]\n",
      " [ 5.77067003e-03  7.23693973e-04  4.81953246e-04 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 8.32661846e-02  6.26742988e-03  2.30605116e-03 ...  0.00000000e+00\n",
      "   0.00000000e+00 -3.54045265e-01]]\n",
      "Initial Output Layer Weights:\n",
      " [[ 0.07786246]\n",
      " [ 0.02736099]\n",
      " [ 0.09948992]\n",
      " [-0.08508564]\n",
      " [ 0.09885941]\n",
      " [ 0.05906969]\n",
      " [ 0.0894683 ]\n",
      " [ 0.07701661]\n",
      " [ 0.03294838]\n",
      " [ 0.00225711]\n",
      " [-0.00887391]\n",
      " [-0.05327769]\n",
      " [ 0.04315051]\n",
      " [-0.07684231]\n",
      " [-0.07372498]\n",
      " [ 0.00571507]\n",
      " [ 0.05005577]\n",
      " [ 0.06596454]\n",
      " [-0.01631921]\n",
      " [-0.02455317]\n",
      " [ 0.0894195 ]\n",
      " [-0.04770501]\n",
      " [ 0.02593371]\n",
      " [-0.0795577 ]\n",
      " [ 0.00780946]\n",
      " [-0.05174792]\n",
      " [-0.00042918]\n",
      " [-0.08091727]\n",
      " [-0.01076104]\n",
      " [-0.00841302]\n",
      " [-0.02966902]\n",
      " [-0.00044082]\n",
      " [-0.07265517]\n",
      " [-0.04382725]\n",
      " [ 0.05698965]\n",
      " [-0.07664341]\n",
      " [ 0.00599703]\n",
      " [ 0.05621793]\n",
      " [-0.07370856]\n",
      " [ 0.00865322]\n",
      " [-0.09708863]\n",
      " [ 0.02158827]\n",
      " [ 0.05482443]\n",
      " [ 0.01213197]\n",
      " [ 0.06373714]\n",
      " [ 0.05485562]\n",
      " [-0.06546518]\n",
      " [ 0.0392989 ]\n",
      " [ 0.09891342]\n",
      " [ 0.05456149]\n",
      " [ 0.0462704 ]\n",
      " [ 0.07713446]\n",
      " [ 0.01538467]\n",
      " [ 0.00083333]\n",
      " [-0.05990688]\n",
      " [ 0.00557152]\n",
      " [ 0.02758231]\n",
      " [-0.07255465]\n",
      " [ 0.00618511]\n",
      " [ 0.09106975]\n",
      " [-0.07781542]\n",
      " [-0.07299835]\n",
      " [ 0.08175523]\n",
      " [-0.02255675]\n",
      " [-0.09122173]\n",
      " [ 0.05152682]\n",
      " [ 0.07660467]\n",
      " [ 0.09215543]\n",
      " [-0.08726417]\n",
      " [-0.05367845]\n",
      " [-0.00754208]\n",
      " [ 0.00969579]\n",
      " [-0.01550695]\n",
      " [ 0.07157755]\n",
      " [ 0.06016929]\n",
      " [-0.06802038]\n",
      " [ 0.02807335]\n",
      " [ 0.06640334]\n",
      " [ 0.05939989]\n",
      " [ 0.06940797]\n",
      " [ 0.09873418]\n",
      " [-0.09792053]\n",
      " [ 0.07575483]\n",
      " [-0.04988026]\n",
      " [ 0.066699  ]\n",
      " [ 0.08272683]\n",
      " [-0.03152735]\n",
      " [ 0.03956149]\n",
      " [-0.07045967]\n",
      " [-0.00301209]\n",
      " [ 0.08568684]\n",
      " [-0.06461043]\n",
      " [-0.09215457]\n",
      " [-0.08760366]\n",
      " [ 0.02386039]\n",
      " [ 0.07726049]\n",
      " [ 0.02271381]\n",
      " [-0.09910292]\n",
      " [-0.08276348]\n",
      " [ 0.09915511]\n",
      " [ 0.06949852]\n",
      " [-0.08431634]\n",
      " [ 0.09461876]\n",
      " [ 0.05547383]\n",
      " [ 0.09343031]\n",
      " [-0.02053386]\n",
      " [-0.05024181]\n",
      " [ 0.04251422]\n",
      " [ 0.05092938]\n",
      " [-0.01019052]\n",
      " [ 0.0329808 ]\n",
      " [ 0.09337601]\n",
      " [-0.08991051]\n",
      " [-0.0708024 ]\n",
      " [-0.05984515]\n",
      " [ 0.08537762]\n",
      " [ 0.02206318]\n",
      " [-0.02398463]\n",
      " [ 0.03272438]\n",
      " [-0.00469279]\n",
      " [-0.03949437]\n",
      " [ 0.04670041]\n",
      " [-0.06371817]\n",
      " [ 0.00037098]\n",
      " [-0.06549501]\n",
      " [-0.03633377]\n",
      " [-0.01773399]\n",
      " [-0.06608529]\n",
      " [ 0.09297804]\n",
      " [ 0.0165376 ]\n",
      " [ 0.05633412]\n",
      " [-0.07649826]\n",
      " [ 0.05700485]\n",
      " [ 0.0292666 ]\n",
      " [-0.09034237]\n",
      " [-0.08277762]\n",
      " [-0.02702669]\n",
      " [-0.05433483]\n",
      " [ 0.06070396]\n",
      " [ 0.06742241]\n",
      " [-0.07190498]\n",
      " [ 0.01421025]\n",
      " [-0.01414063]\n",
      " [-0.01322906]\n",
      " [-0.03555426]\n",
      " [ 0.02058356]\n",
      " [-0.07080407]\n",
      " [-0.09489382]\n",
      " [ 0.09496206]\n",
      " [ 0.04400447]\n",
      " [-0.01195806]\n",
      " [-0.03146486]\n",
      " [ 0.07736711]\n",
      " [ 0.07026036]\n",
      " [-0.08709104]\n",
      " [-0.01103836]\n",
      " [-0.08596527]\n",
      " [ 0.06264023]\n",
      " [ 0.09329741]\n",
      " [-0.02131104]\n",
      " [ 0.0843004 ]\n",
      " [ 0.03758561]\n",
      " [-0.07943686]\n",
      " [-0.07092749]\n",
      " [ 0.06369535]\n",
      " [-0.05412007]\n",
      " [ 0.01921549]\n",
      " [-0.08621327]\n",
      " [-0.05870137]\n",
      " [ 0.0838973 ]\n",
      " [ 0.04275084]\n",
      " [ 0.08917461]\n",
      " [-0.07789045]\n",
      " [-0.01789779]\n",
      " [ 0.05545485]\n",
      " [ 0.04137096]\n",
      " [ 0.03229334]\n",
      " [ 0.08215396]\n",
      " [-0.06222131]\n",
      " [ 0.03752236]\n",
      " [ 0.07247319]\n",
      " [ 0.02084714]\n",
      " [-0.08234584]\n",
      " [ 0.0080925 ]\n",
      " [ 0.00853885]\n",
      " [-0.08341491]\n",
      " [-0.07585073]\n",
      " [ 0.05652765]\n",
      " [-0.06194749]\n",
      " [ 0.04834578]\n",
      " [ 0.07219319]\n",
      " [-0.02073323]\n",
      " [-0.02377153]\n",
      " [-0.0305303 ]\n",
      " [-0.06448937]\n",
      " [-0.07085287]\n",
      " [ 0.08534924]\n",
      " [-0.09812959]\n",
      " [-0.08499587]\n",
      " [-0.01690803]]\n",
      "Initial Hidden Layer Weights:\n",
      " [[ 0.07249725  0.0032965   0.08662805 ... -0.04999404  0.0819602\n",
      "  -0.07665769]\n",
      " [ 0.08721366  0.00317062 -0.05763957 ...  0.04865724  0.08266704\n",
      "   0.08170818]\n",
      " [-0.0548368   0.00346612 -0.05208727 ... -0.04406497 -0.09951739\n",
      "  -0.09630107]\n",
      " ...\n",
      " [ 0.08274465 -0.00309592 -0.07181215 ...  0.00855071 -0.09526546\n",
      "   0.06443683]\n",
      " [-0.06710274  0.08498413  0.05649227 ... -0.0047521   0.01411031\n",
      "  -0.00156799]\n",
      " [ 0.02598364  0.02412194  0.06695122 ...  0.05261329 -0.08838132\n",
      "   0.02167602]]\n",
      "Initial Output Layer Bias:\n",
      " [[-1.]]\n",
      "Initial Hidden Layer Bias:\n",
      " [[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
      "  -1. -1.]]\n",
      "Final Output Layer Weights:\n",
      " [[ 7.78624584e-02]\n",
      " [ 6.88824113e-02]\n",
      " [ 9.94899222e-02]\n",
      " [-8.50856443e-02]\n",
      " [ 1.03171582e-01]\n",
      " [ 2.03610492e-01]\n",
      " [ 8.94682997e-02]\n",
      " [ 7.70166111e-02]\n",
      " [ 3.29483832e-02]\n",
      " [ 2.26468973e-03]\n",
      " [-8.87391301e-03]\n",
      " [-5.32776923e-02]\n",
      " [ 4.74246197e-02]\n",
      " [-7.68423127e-02]\n",
      " [-7.37249797e-02]\n",
      " [ 2.96281686e-02]\n",
      " [ 5.00557652e-02]\n",
      " [ 6.59645359e-02]\n",
      " [-1.63192112e-02]\n",
      " [-2.38337712e-02]\n",
      " [ 8.94194989e-02]\n",
      " [-4.77050128e-02]\n",
      " [ 3.73235793e-02]\n",
      " [-7.95559070e-02]\n",
      " [ 7.80945590e-03]\n",
      " [-5.17479198e-02]\n",
      " [ 3.92295547e-02]\n",
      " [-8.09172703e-02]\n",
      " [-1.07610446e-02]\n",
      " [-8.41302087e-03]\n",
      " [-2.96690152e-02]\n",
      " [-4.40821533e-04]\n",
      " [-7.26551709e-02]\n",
      " [-3.98938271e-02]\n",
      " [ 9.93469417e-02]\n",
      " [-4.00958841e-02]\n",
      " [ 5.99703379e-03]\n",
      " [ 5.62179295e-02]\n",
      " [-7.36990167e-02]\n",
      " [ 8.65321934e-03]\n",
      " [-9.08355129e-02]\n",
      " [ 8.82083230e-02]\n",
      " [ 5.48277285e-02]\n",
      " [ 2.61495191e-02]\n",
      " [ 6.37371434e-02]\n",
      " [ 5.48556194e-02]\n",
      " [-6.54651833e-02]\n",
      " [ 3.92988953e-02]\n",
      " [ 1.86813195e-01]\n",
      " [ 5.45614916e-02]\n",
      " [ 6.90306688e-02]\n",
      " [ 7.71344635e-02]\n",
      " [ 4.19001214e-02]\n",
      " [ 8.33329182e-04]\n",
      " [-5.78648559e-02]\n",
      " [ 5.57152393e-03]\n",
      " [ 2.75823101e-02]\n",
      " [-7.25546479e-02]\n",
      " [ 6.18511317e-03]\n",
      " [ 9.10697488e-02]\n",
      " [-7.78032556e-02]\n",
      " [-7.29983465e-02]\n",
      " [ 8.17552335e-02]\n",
      " [-1.75560217e-02]\n",
      " [-8.43574622e-02]\n",
      " [ 5.15270379e-02]\n",
      " [ 7.66046694e-02]\n",
      " [ 9.21554287e-02]\n",
      " [-8.72641734e-02]\n",
      " [-5.36784507e-02]\n",
      " [ 1.23953224e-03]\n",
      " [ 1.89402566e-02]\n",
      " [-3.76794659e-03]\n",
      " [ 7.47173942e-02]\n",
      " [ 6.01692934e-02]\n",
      " [-6.80203795e-02]\n",
      " [ 2.80733524e-02]\n",
      " [ 6.64033388e-02]\n",
      " [ 5.94012231e-02]\n",
      " [ 6.94079715e-02]\n",
      " [ 9.87341757e-02]\n",
      " [-9.79205266e-02]\n",
      " [ 7.57548289e-02]\n",
      " [ 7.52208653e-03]\n",
      " [ 6.66989980e-02]\n",
      " [ 8.27344232e-02]\n",
      " [-3.15193851e-02]\n",
      " [ 5.43435939e-02]\n",
      " [-6.56377226e-02]\n",
      " [-2.84072815e-03]\n",
      " [ 3.87329810e-01]\n",
      " [-6.46104251e-02]\n",
      " [-9.21508849e-02]\n",
      " [-8.76036591e-02]\n",
      " [ 2.38727543e-02]\n",
      " [ 7.72696714e-02]\n",
      " [ 2.27138127e-02]\n",
      " [-1.49789833e-02]\n",
      " [-8.27634764e-02]\n",
      " [ 9.91551067e-02]\n",
      " [ 9.09043963e-02]\n",
      " [-8.43163357e-02]\n",
      " [ 9.46187588e-02]\n",
      " [ 5.54738274e-02]\n",
      " [ 9.34303111e-02]\n",
      " [ 8.85121002e-02]\n",
      " [-5.02315824e-02]\n",
      " [ 4.66493995e-02]\n",
      " [ 5.09293786e-02]\n",
      " [-1.01905217e-02]\n",
      " [ 3.29808046e-02]\n",
      " [ 2.06979165e-01]\n",
      " [-8.99105075e-02]\n",
      " [-7.08023955e-02]\n",
      " [-5.98451451e-02]\n",
      " [ 1.15487260e-01]\n",
      " [ 5.22345058e-02]\n",
      " [-2.39846342e-02]\n",
      " [ 3.27313516e-02]\n",
      " [ 8.87083344e-02]\n",
      " [-4.51806248e-03]\n",
      " [ 4.67039660e-02]\n",
      " [-4.36832832e-02]\n",
      " [ 3.71044000e-04]\n",
      " [-6.54950065e-02]\n",
      " [-3.10613509e-02]\n",
      " [-1.77216360e-02]\n",
      " [-6.60852920e-02]\n",
      " [ 1.98399316e-01]\n",
      " [ 1.92758050e-02]\n",
      " [ 5.63341195e-02]\n",
      " [-5.16155690e-02]\n",
      " [ 5.70048544e-02]\n",
      " [ 2.92751100e-02]\n",
      " [-9.03423686e-02]\n",
      " [-8.27776202e-02]\n",
      " [-2.70266924e-02]\n",
      " [-5.43348262e-02]\n",
      " [ 6.72883503e-02]\n",
      " [ 6.74224078e-02]\n",
      " [-7.19049757e-02]\n",
      " [ 3.04749431e-02]\n",
      " [-1.41406331e-02]\n",
      " [-1.32290626e-02]\n",
      " [-2.58075916e-02]\n",
      " [ 3.15973443e-02]\n",
      " [-4.39956768e-02]\n",
      " [-9.48911319e-02]\n",
      " [ 1.10846474e-01]\n",
      " [ 6.20332865e-02]\n",
      " [ 2.36275758e-01]\n",
      " [-3.14641466e-02]\n",
      " [ 1.50040849e-01]\n",
      " [ 7.02603629e-02]\n",
      " [-8.70910432e-02]\n",
      " [-1.10383593e-02]\n",
      " [-8.59652681e-02]\n",
      " [ 7.45244194e-02]\n",
      " [ 9.32974103e-02]\n",
      " [-2.13110410e-02]\n",
      " [ 8.43004019e-02]\n",
      " [ 3.75856054e-02]\n",
      " [-7.41007502e-02]\n",
      " [-6.95095405e-02]\n",
      " [ 6.36953495e-02]\n",
      " [-3.87640520e-02]\n",
      " [ 2.89604970e-02]\n",
      " [-8.61823050e-02]\n",
      " [ 1.19881898e-02]\n",
      " [ 8.38972972e-02]\n",
      " [ 5.34479812e-02]\n",
      " [ 1.68274361e-01]\n",
      " [-5.68710619e-02]\n",
      " [-1.78725465e-02]\n",
      " [ 5.54548462e-02]\n",
      " [ 4.13709621e-02]\n",
      " [ 3.55070754e-02]\n",
      " [ 8.21539582e-02]\n",
      " [-4.71993773e-02]\n",
      " [ 7.46866080e-02]\n",
      " [ 8.05593343e-02]\n",
      " [ 2.08471383e-02]\n",
      " [-8.23458370e-02]\n",
      " [ 8.09250160e-03]\n",
      " [ 8.53885092e-03]\n",
      " [-2.12690182e-02]\n",
      " [-7.58507311e-02]\n",
      " [ 5.65276516e-02]\n",
      " [-6.19474872e-02]\n",
      " [ 4.83457800e-02]\n",
      " [ 8.95671553e-02]\n",
      " [-2.07332278e-02]\n",
      " [-2.37715268e-02]\n",
      " [-1.46989749e-02]\n",
      " [-6.44893738e-02]\n",
      " [-7.08528705e-02]\n",
      " [ 1.23272516e-01]\n",
      " [-8.15837662e-02]\n",
      " [-8.49958665e-02]\n",
      " [-1.14961733e-02]]\n",
      "Final Hidden Layer Weights:\n",
      " [[ 0.07249725  0.00441944  0.08662805 ... -0.05039978  0.0819602\n",
      "  -0.07687801]\n",
      " [ 0.08721366  0.00317851 -0.05763957 ...  0.04865437  0.08266704\n",
      "   0.08170802]\n",
      " [-0.0548368   0.00348288 -0.05208727 ... -0.0440667  -0.09951739\n",
      "  -0.09630733]\n",
      " ...\n",
      " [ 0.08274465 -0.00309172 -0.07181215 ...  0.00854785 -0.09526546\n",
      "   0.06443543]\n",
      " [-0.06710274  0.08498946  0.05649227 ... -0.00475418  0.01411031\n",
      "  -0.00156999]\n",
      " [ 0.02598364  0.02371769  0.06695122 ...  0.05261044 -0.08838132\n",
      "   0.02167421]]\n",
      "Final Output Layer Bias:\n",
      " [[-0.75811209]]\n",
      "Final Hidden Layer Bias:\n",
      " [[-0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224 -0.86958224\n",
      "  -0.86958224 -0.86958224]]\n",
      "Target Output:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "After Training:\n",
      " [[0.99635896]\n",
      " [0.99508716]\n",
      " [0.99403086]\n",
      " [0.99067092]\n",
      " [0.98263074]\n",
      " [0.98947825]\n",
      " [0.98884762]\n",
      " [0.99228904]\n",
      " [0.98953561]\n",
      " [0.98707844]]\n",
      "Human detected and the predicted output is [0.99635896]\n",
      "Human detected and the predicted output is [0.99508716]\n",
      "Human detected and the predicted output is [0.99403086]\n",
      "Human detected and the predicted output is [0.99067092]\n",
      "Human detected and the predicted output is [0.98263074]\n",
      "Human detected and the predicted output is [0.98947825]\n",
      "Human detected and the predicted output is [0.98884762]\n",
      "Human detected and the predicted output is [0.99228904]\n",
      "Human detected and the predicted output is [0.98953561]\n",
      "Human detected and the predicted output is [0.98707844]\n",
      "Average error =  [[0.10850802]\n",
      " [0.11430679]\n",
      " [0.11005945]\n",
      " [0.12594249]\n",
      " [0.13935049]\n",
      " [0.12071118]\n",
      " [0.12752835]\n",
      " [0.11885053]\n",
      " [0.12383955]\n",
      " [0.13829678]]\n"
     ]
    }
   ],
   "source": [
    "class Driver:\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        train_path_positive = \"C://Users//anvit//OneDrive//Desktop//HoG//Training_images(Pos)\"\n",
    "        train_path_negative = \"C://Users//anvit//OneDrive//Desktop//HoG//Training_images(Neg)\"\n",
    "        test_neg = \"C://Users//anvit//OneDrive//Desktop//HoG//Test_images(Neg)\"\n",
    "        test_pos = \"C://Users//anvit//OneDrive//Desktop//HoG//Test_images(Pos)\"\n",
    "        \n",
    "        def gen_hog_vectors(path):\n",
    "            vectors = []\n",
    "            files = os.listdir(path)\n",
    "            for file in files:\n",
    "                print(file)\n",
    "                t = Preprocessing(path + '//' + file)\n",
    "                image = t.img_array\n",
    "#         print(image)\n",
    "                gray = t.gray_array\n",
    "#         print(gray)\n",
    "                g_x = t.g_x\n",
    "#         print(g_x[28][28])\n",
    "                g_y = t.g_y\n",
    "#         print(g_y[28][28])\n",
    "                g_mag = t.g_mag\n",
    "#         print(g_mag[28][28])\n",
    "                theta = t.theta\n",
    "#         print(theta[28][28])\n",
    "                h_feature_output = np.array(h_feature(g_mag,theta))\n",
    "                print(h_feature_output.shape)\n",
    "#                 h_feature_output = h_feature(g_mag,theta)\n",
    "                vectors.append(h_feature_output)\n",
    "                result = np.array(vectors)\n",
    "                print(result)\n",
    "            return result\n",
    "        \n",
    "        def train_nn(inputs,target_output):\n",
    "            \n",
    "            epochs = 10\n",
    "            alpha = 0.1\n",
    "            input_layer_neurons = 7524\n",
    "            hidden_layer_neurons = 200\n",
    "            output_layer_neurons = 1\n",
    "            err = 0\n",
    "            \n",
    "#             inputs = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "#             target_output = np.array([[0],[1],[1],[0]]) \n",
    "            \n",
    "            # Step 1: Initialize the weights and biases to have random values between âˆ’0.5 and 0.5:\n",
    "            # weights\n",
    "            hidden_layer_weights = np.random.uniform(-0.5,0.5, size = (input_layer_neurons,hidden_layer_neurons)) * 2 * 0.1\n",
    "            output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons)) * 2 * 0.1\n",
    "\n",
    "            # bias:\n",
    "            hidden_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,hidden_layer_neurons))\n",
    "            output_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,output_layer_neurons))\n",
    "\n",
    "            print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "                \n",
    "            # Step 2: Training the inputs\n",
    "\n",
    "            # Step 2a:\n",
    "            # Setting the corresponding activation(a1) for input_layer\n",
    "            def sigmoid(x):\n",
    "                return 1/(1 + np.exp(-x))\n",
    "\n",
    "            # We need the derivative of this for further calculations\n",
    "            def derivative_of_sigmoid(x):\n",
    "            #     derivative of sigmoid in terms of itslef is:\n",
    "                return x * (1 - x)\n",
    "            \n",
    "            def relu(x):\n",
    "                y = x.copy()\n",
    "                y[x<0] = 0\n",
    "                return y\n",
    "        \n",
    "            # Derivative of ReLU in terms of itself is:    \n",
    "            def derivative_of_relu(x):\n",
    "                y = x.copy()\n",
    "                y[x>0] = 1\n",
    "                y[x<=0] = 0\n",
    "                return y\n",
    "\n",
    "            for i in range(epochs):\n",
    "                #     Step 2b:\n",
    "                #     Feedforward\n",
    "                h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "                h_layer_a = h_layer_a + hidden_layer_bias\n",
    "                output_hidden_layer = relu(h_layer_a)\n",
    "    \n",
    "                o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "                o_layer_a = o_layer_a + output_layer_bias\n",
    "                pred_output = sigmoid(o_layer_a)\n",
    "\n",
    "                #     Step 2c:\n",
    "                #     Output error\n",
    "                \n",
    "                final_error = target_output - pred_output\n",
    "                err = err + np.absolute(target_output - pred_output)\n",
    "                pred_output_delta = final_error * derivative_of_sigmoid(pred_output)\n",
    "\n",
    "                #     Step 2d:\n",
    "                #     Backpropogating the error\n",
    "                h_layer_error = pred_output_delta.dot(output_layer_weights.T)\n",
    "                hidden_layer_output_delta = h_layer_error * derivative_of_relu(output_hidden_layer)\n",
    "\n",
    "                #     Step 2e:\n",
    "                #     Updating weights and bias\n",
    "                output_layer_weights = output_layer_weights + output_hidden_layer.T.dot(pred_output_delta) * alpha\n",
    "                output_layer_bias = output_layer_bias + np.sum(pred_output_delta) * alpha\n",
    "    \n",
    "                hidden_layer_weights = hidden_layer_weights + inputs.T.dot(hidden_layer_output_delta) * alpha\n",
    "                hidden_layer_bias = hidden_layer_bias + np.sum(hidden_layer_output_delta) * alpha\n",
    "\n",
    "            print('Final Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Final Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Final Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Final Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "\n",
    "            print('Target Output:\\n', target_output)\n",
    "            print('After Training:\\n', pred_output)\n",
    "            for i in range(pred_output.shape[0]):\n",
    "                if(pred_output[i] >= 0.6):\n",
    "                    print('Human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] > 0.4 and pred_output[i] < 0.6):\n",
    "                    print('Boarderline human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] <= 0.4):\n",
    "                    print('No human detected and the predicted output is', pred_output[i])\n",
    "            print('Average error = ', err/ (inputs.shape[0]))\n",
    "                    \n",
    "        def test_nn(inputs,target_output):\n",
    "            \n",
    "            epochs = 10\n",
    "            alpha = 0.1\n",
    "            input_layer_neurons = 7524\n",
    "            hidden_layer_neurons = 200\n",
    "            output_layer_neurons = 1\n",
    "            err = 0\n",
    "            \n",
    "#             inputs = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "#             target_output = np.array([[0],[1],[1],[0]]) \n",
    "            \n",
    "            # Step 1: Initialize the weights and biases to have random values between âˆ’0.5 and 0.5:\n",
    "            # weights\n",
    "            hidden_layer_weights = np.random.uniform(-0.5,0.5, size = (input_layer_neurons,hidden_layer_neurons)) * 2 * 0.1\n",
    "            output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons)) * 2 * 0.1\n",
    "\n",
    "            # bias:\n",
    "            hidden_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,hidden_layer_neurons))\n",
    "            output_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,output_layer_neurons))\n",
    "\n",
    "            print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "                \n",
    "            # Step 2: Training the inputs\n",
    "\n",
    "            # Step 2a:\n",
    "            # Setting the corresponding activation(a1) for input_layer\n",
    "            def sigmoid(x):\n",
    "                return 1/(1 + np.exp(-x))\n",
    "\n",
    "            # We need the derivative of this for further calculations\n",
    "            def derivative_of_sigmoid(x):\n",
    "            #     derivative of sigmoid in terms of itslef is:\n",
    "                return x * (1 - x)\n",
    "            \n",
    "            def relu(x):\n",
    "                y = x.copy()\n",
    "                y[x<0] = 0\n",
    "                return y\n",
    "        \n",
    "            # Derivative of ReLU in terms of itself is:    \n",
    "            def derivative_of_relu(x):\n",
    "                y = x.copy()\n",
    "                y[x>0] = 1\n",
    "                y[x<=0] = 0\n",
    "                return y\n",
    "\n",
    "#             for i in range(epochs):\n",
    "                #     Step 2b:\n",
    "                #     Feedforward\n",
    "            h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "            h_layer_a = h_layer_a + hidden_layer_bias\n",
    "            output_hidden_layer = relu(h_layer_a)\n",
    "    \n",
    "            o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "            o_layer_a = o_layer_a + output_layer_bias\n",
    "            pred_output = sigmoid(o_layer_a)\n",
    "            \n",
    "            print('Target Output:\\n', target_output)\n",
    "            print('After Training:\\n', pred_output)\n",
    "            for i in range(pred_output.shape[0]):\n",
    "                if(pred_output[i] >= 0.6):\n",
    "                    print('Human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] > 0.4 and pred_output[i] < 0.6):\n",
    "                    print('Boarderline human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] <= 0.4):\n",
    "                    print('No human detected and the predicted output is', pred_output[i])\n",
    "#             print('Average error = ', err/ (inputs.shape[0]))\n",
    "        \n",
    "        positive_hog_feature_vectors = gen_hog_vectors(train_path_positive)\n",
    "        pos_train = train_nn(positive_hog_feature_vectors, target_output = np.full((10,1), 1.0, dtype = float))\n",
    "#         negative_hog_feature_vectors = gen_hog_vectors(train_path_negative)\n",
    "#         neg_train = train_nn(negative_hog_feature_vectors, target_output = np.full((10,1), 0.0, dtype = float))\n",
    "#         positive_test_features = gen_hog_vectors(test_pos)\n",
    "#         pos_test = test_nn(positive_test_features, target_output = np.full((5,1), 1.0, dtype = float))\n",
    "#         negative_test_features = gen_hog_vectors(test_neg)\n",
    "#         neg_test = train_nn(negative_test_features, target_output = np.full((5,1), 0.0, dtype = float))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00018105, 0.00353208],\n",
       "       [0.00634868, 0.00545268],\n",
       "       [0.00541303, 0.00525631]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,2)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.38252536, -0.18969548],\n",
       "       [-0.21256867,  0.00360878],\n",
       "       [-0.46063689, -0.02509898]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (0.5 - (-0.5))* np.random.random_sample((3, 2)) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
