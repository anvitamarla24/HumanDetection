{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.img = np.full((160, 96), 0)\n",
    "        self.img_array = np.full((160, 96), 0)\n",
    "        self.gray = np.full((160, 96), 0)\n",
    "        self.gray_array = np.full((160, 96), 0)\n",
    "        \n",
    "        self.G_X = np.full((160, 96), 0)\n",
    "        self.G_Y = np.full((160, 96), 0)\n",
    "        self.g_x = np.full((160, 96), 0)\n",
    "        self.g_y = np.full((160, 96), 0)\n",
    "        self.g_mag = np.full((160, 96), 0)\n",
    "        self.sobel_image = np.full((160, 96), 0)\n",
    "        \n",
    "        self.theta = np.full((160, 96), 0)       \n",
    "        \n",
    "        self.path = path\n",
    "        self.img_array = np.full((160, 96), 0)\n",
    "        self.image_read()\n",
    "        self.gray = self.rgb2gray(self.img_array)\n",
    "        self.image_write(self.gray)\n",
    "        \n",
    "        self.g_x, self.g_y, self.sobel_output = np.round(self.sobels_operator(self.gray))\n",
    "        self.theta = self.gradient_angles(self.gray_array, self.g_x, self.g_y)\n",
    "#         print(self.g_x[28][28])\n",
    "#         print(self.g_y[28][28])\n",
    "#         print(self.theta[28][28])\n",
    "        \n",
    "#         return  self.img_array, self.gray_array, self.g_mag, self.theta\n",
    "        \n",
    "#         self\n",
    "    \n",
    "    def image_read(self):\n",
    "        self.img = cv2.imread(self.path)  \n",
    "        self.img_array = np.array(self.img, dtype=float)\n",
    "#         self.gray = self.rgb2gray(self.img)\n",
    "#         self.\n",
    "    \n",
    "    def rgb2gray(self, rgb):\n",
    "        return np.round(np.dot(rgb[...,:3], [0.299, 0.587, 0.114]))\n",
    "    \n",
    "    def image_write(self, gray):\n",
    "        self.img_gray = cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray.bmp', self.gray)\n",
    "        self.gray_array = np.array(gray, dtype=float)\n",
    "#         return self.gray_array\n",
    "\n",
    "    #sobels operator\n",
    "    def sobels_operator(self, inp):\n",
    "        self.inp = inp\n",
    "        g_x = np.array([[-1,0,1],\n",
    "                       [-2,0,2],\n",
    "                       [-1,0,1]])\n",
    "        g_y = np.array([[1,2,1],\n",
    "                       [0,0,0],\n",
    "                       [-1,-2,-1]])\n",
    "    \n",
    "        #Horizontal Gradient \n",
    "        self.G_X = self.conv_sobel(self.inp,g_x)\n",
    "        #Saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobelGX.bmp', self.G_X)\n",
    "       \n",
    "        #Vertical Gradient\n",
    "        self.G_Y = self.conv_sobel(self.inp,g_y)\n",
    "        #saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobelGY.bmp', self.G_Y)\n",
    "    \n",
    "        #Gradient Magnitude\n",
    "        self.g_mag = np.sqrt(np.square(self.G_X) + np.square(self.G_Y))\n",
    "        #saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobel.bmp', self.g_mag)    \n",
    "        return self.G_X, self.G_Y, self.g_mag\n",
    "\n",
    "    #convolution done by implementing using 2D directly\n",
    "    def conv_sobel(self,x,y):\n",
    "        \n",
    "        row_size = x.shape[0]\n",
    "        column_size = x.shape[1]        \n",
    "        conv_img = np.zeros((row_size, column_size), dtype=np.float)\n",
    "        \n",
    "        for a in range(0, row_size - 2):\n",
    "            for b in range(0, column_size - 2):\n",
    "                \n",
    "                conv_img[a + 1][b + 1] = (np.sum(x[a: a + 3, b: b + 3] * y)) / 4\n",
    "\n",
    "        return conv_img\n",
    "    \n",
    "    def gradient_angles(self, gray_array, g_x, g_y):\n",
    "        #Finding the Gradient Angles i.e., theta\n",
    "        print(self.gray_array.shape[0])\n",
    "        #arctan2 is tan inverse, which gives us results in radians with [-pi to pi]\n",
    "        self.theta = np.zeros((self.gray_array.shape[0],self.gray_array.shape[1]))\n",
    "        for i in range(self.gray_array.shape[0]):\n",
    "            for j in range(0, self.gray_array.shape[1]):\n",
    "                if(self.g_y[i][j] == 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = 0\n",
    "                elif(self.g_y[i][j] > 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = 90\n",
    "                elif(self.g_y[i][j] < 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = -90\n",
    "                elif(self.g_y[i][j] == 0):\n",
    "                    self.theta[i][j] = 0\n",
    "                else:\n",
    "                    if(self.g_x[i][j] == 0):\n",
    "                        print('here')\n",
    "                    self.theta[i][j] = np.arctan2(self.g_y[i][j],self.g_x[i][j])\n",
    "    \n",
    "        #Converting radians to degrees\n",
    "        self.theta = np.rad2deg(self.theta)\n",
    "        #converting all the negatives into positives by adding 360 so the range is now [0 to 360]\n",
    "        for i in range(0, self.gray_array.shape[0]):\n",
    "            for j in range(0, self.gray_array.shape[1]):\n",
    "                if(self.theta[i][j] < 0):\n",
    "                    self.theta[i][j] += 360\n",
    "                    \n",
    "        return self.theta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram:\n",
    "    def __init__(self, mag_block, theta_block):\n",
    "        self.center = np.array([0,20,40,60,80,100,120,140,160])\n",
    "#         4 histograms of 9 bins each - per block\n",
    "        self.bins = np.zeros((4,9), dtype=float)\n",
    "        self.cell_size = 8\n",
    "        self.cells = []\n",
    "        self.t_cells = []\n",
    "        self.m_cells = []\n",
    "        self.flattened = []\n",
    "        self.theta_block = theta_block\n",
    "        self.mag_block = mag_block\n",
    "        self.convert_block_2_cell(mag_block,theta_block)\n",
    "#         c=0\n",
    "#         print(c)\n",
    "        self.generate_features(self.m_cells, self.t_cells)\n",
    "        self.norm_hog()\n",
    "        self.flattened_bins()\n",
    "        \n",
    "    def flattened_bins(self):\n",
    "        for bin in self.bins:\n",
    "            for item in bin:\n",
    "                self.flattened.append(item)\n",
    "        \n",
    "    \n",
    "    def norm_hog(self):\n",
    "        \n",
    "        summation = 0\n",
    "        for bin in self.bins:\n",
    "            for j in range(len(bin)):\n",
    "                summation += bin[j] ** 2\n",
    "        \n",
    "        dist = summation ** 0.5\n",
    "                   \n",
    "        for bin in self.bins:\n",
    "            for j in range(len(bin)):\n",
    "                if(dist == 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    bin[j] /= dist\n",
    "\n",
    "    \n",
    "    def convert_block_2_cell(self, mag_block, theta_block):\n",
    "        c=0\n",
    "        for m in range(0,theta_block.shape[0], self.cell_size):\n",
    "                for n in range(0, theta_block.shape[1], self.cell_size):\n",
    "                    \n",
    "                    m_cell = np.zeros((self.cell_size,self.cell_size))\n",
    "                    t_cell = np.zeros((self.cell_size,self.cell_size))\n",
    "                    \n",
    "                    for p in range(self.cell_size):\n",
    "                        for q in range(self.cell_size):\n",
    "                            \n",
    "                            m_cell[p][q] = mag_block[m+p][n+q]\n",
    "                            t_cell[p][q] = theta_block[m+p][n+q]\n",
    "                     \n",
    "                    c+=1\n",
    "                    self.m_cells.append(m_cell)\n",
    "                    self.t_cells.append(t_cell)\n",
    "#         print(c)\n",
    "    \n",
    "    def generate_features(self, m_cells, t_cells):\n",
    "        \n",
    "        for i in range(len(t_cells)):\n",
    "            for j in range(len(t_cells[i])):\n",
    "                for k in range(len(t_cells[i][j])):\n",
    "                    self.cal_hist(m_cells[i][j][k],t_cells[i][j][k], self.bins[i]) \n",
    "                \n",
    "# HANDLE EDGE CASE : -20,10       \n",
    "    def cal_hist(self, mag, angle, bins):       \n",
    "        \n",
    "#         COMPUTE THE DISTANCE TO ADD THE MAG WRT THE CENTER\n",
    "        if(angle >= 180):\n",
    "            angle -= 180\n",
    "#         now we have all unsigned angles\n",
    "# new case to consider: if angle is >=160 first = bin 9 and second is bin 1\n",
    "        \n",
    "        if(angle >= 160):\n",
    "            first_bin = 8\n",
    "            second_bin = 0\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag \n",
    "#         edge care , if angle is >=350 it's between bins 1 and 9\n",
    "        \n",
    "        if(angle >= 0 and angle < 20):\n",
    "            first_bin = 0\n",
    "            second_bin = 1\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 20 and angle < 40):\n",
    "            first_bin = 1\n",
    "            second_bin = 2\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "        if(angle >= 40 and angle < 60):\n",
    "            first_bin = 2\n",
    "            second_bin = 3\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "        if(angle >= 60 and angle < 80):\n",
    "            first_bin = 3\n",
    "            second_bin = 4\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 80 and angle < 100):\n",
    "            first_bin = 4\n",
    "            second_bin = 5\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 100 and angle < 120):\n",
    "            first_bin = 5\n",
    "            second_bin = 6\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 120 and angle < 140):\n",
    "            first_bin = 6\n",
    "            second_bin = 7\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "         \n",
    "        if(angle >= 140 and angle < 160):\n",
    "            first_bin = 7\n",
    "            second_bin = 8\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "    def calc_distance(self, angle, center):        \n",
    "        percent = (np.absolute(angle - center))/20\n",
    "        return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_feature(mag, theta): \n",
    "    descriptor = []\n",
    "    cell_size = 8\n",
    "    block_size = 16\n",
    "    block_overlap = 8\n",
    "    assert(mag.shape[0] == theta.shape[0])\n",
    "    assert(mag.shape[1] == theta.shape[1])\n",
    "    b=0\n",
    "    c=0\n",
    "    \n",
    "#     generating blocks from pixels\n",
    "    for i in range(0,theta.shape[0]-cell_size, cell_size):\n",
    "        for j in range(0,theta.shape[1]-cell_size, cell_size):\n",
    "            \n",
    "            m_block = np.zeros((block_size,block_size))\n",
    "            t_block = np.zeros((block_size,block_size))\n",
    "            \n",
    "            for k in range(block_size):\n",
    "                for l in range(block_size):  \n",
    "                    \n",
    "                    m_block[k][l] = mag[i+k][j+l]\n",
    "                    t_block[k][l] = theta[i+k][j+l]\n",
    "            b +=1\n",
    "\n",
    "            hist_obj = Histogram(m_block,t_block)\n",
    "            for item in hist_obj.flattened:\n",
    "                descriptor.append(item)\n",
    "#     print(b)\n",
    "#     print(c)\n",
    "    return descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_feature_output = np.array(h_feature(sobel_output,theta))\n",
    "# print(np.concatenate(h_feature_output).ravel().tolist())\n",
    "# print(h_feature_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBP:\n",
    "    def __init__(self, gray_block):\n",
    "        self.mag = gray_block\n",
    "        self.bins = np.zeros((59,), dtype=float)\n",
    "        self.generate_features(gray_block)\n",
    "        self.norm_lbp()\n",
    "        \n",
    "    def norm_lbp(self):\n",
    "        for i in range(len(self.bins)):\n",
    "            self.bins[i] = self.bins[i] / 256\n",
    "    \n",
    "    def generate_features(self,gray_block):\n",
    "        for i in range(gray_block.shape[0]):\n",
    "            for j in range(gray_block.shape[1]):\n",
    "                self.compute_LBP_pattern(i,j,gray_block)\n",
    "        \n",
    "#     def norm_lbp(self):\n",
    "#         for i in range(len(self.bins)):\n",
    "#             self.bins[i] = self.bins[i] / 256\n",
    "        \n",
    "    def compute_LBP_pattern(self,i, j,gray_block):\n",
    "        x = gray_block\n",
    "        pattern = ''\n",
    "        \n",
    "        try:\n",
    "            if(x[i-1][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i-1][j] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i-1][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i+1][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i+1][j] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i+1][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            self.append_to_bins(pattern)\n",
    "            \n",
    "        except IndexError:\n",
    "            pattern = '00000101'\n",
    "            self.append_to_bins(pattern)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def append_to_bins(self, lbp_pattern):        \n",
    "        decimal = int(lbp_pattern, 2)\n",
    "        patterns = {\n",
    "            0: 0,\n",
    "            1: 1,\n",
    "            2: 2,\n",
    "            3: 3,\n",
    "            4: 4,\n",
    "            6: 5,\n",
    "            7: 6,\n",
    "            8: 7,\n",
    "            12: 8,\n",
    "            14: 9,\n",
    "            15: 10,\n",
    "            16: 11,\n",
    "            24: 12,\n",
    "            28: 13,\n",
    "            30: 14,\n",
    "            31: 15,\n",
    "            32: 16,\n",
    "            48: 17,\n",
    "            56: 18,\n",
    "            60: 19,\n",
    "            62: 20,\n",
    "            63: 21,\n",
    "            64: 22,\n",
    "            96: 23,\n",
    "            112: 24,\n",
    "            120: 25,\n",
    "            124: 26,\n",
    "            126: 27,\n",
    "            127: 28,\n",
    "            128: 29,\n",
    "            129: 30,\n",
    "            131: 31,\n",
    "            135: 32,\n",
    "            143: 33,\n",
    "            159: 34,\n",
    "            191: 35,\n",
    "            192: 36,\n",
    "            193: 37,\n",
    "            195: 38,\n",
    "            199: 39,\n",
    "            207: 40,\n",
    "            223: 41,\n",
    "            224: 42,\n",
    "            225: 43,\n",
    "            227: 44,\n",
    "            231: 45,\n",
    "            239: 46,\n",
    "            240: 47,\n",
    "            241: 48,\n",
    "            243: 49,\n",
    "            247: 50,\n",
    "            248: 51,\n",
    "            249: 52,\n",
    "            251: 53,\n",
    "            252: 54,\n",
    "            253: 55,\n",
    "            254: 56,\n",
    "            255: 57\n",
    "        }\n",
    "        bin_number = patterns.get(decimal, 58)\n",
    "        self.bins[bin_number] += 1   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LBP_feature(gray):\n",
    "    lbp_descriptor = []\n",
    "    cell_size = 8\n",
    "    block_size = 16\n",
    "    block_overlap = 8\n",
    "    b=0\n",
    "#     c=0\n",
    "    \n",
    "#     generating blocks from pixels   \n",
    "    for i in range(0,gray.shape[0],block_size):\n",
    "        for j in range(0,gray.shape[1],block_size):\n",
    "            \n",
    "            gray_block = np.zeros((block_size,block_size))\n",
    "            \n",
    "            for p in range(block_size):\n",
    "                for q in range(block_size):\n",
    "                    \n",
    "                    gray_block[p][q] = gray[i+p][j+q]\n",
    "            \n",
    "            \n",
    "            b+=1\n",
    "            lbp_obj1 = LBP(gray_block)\n",
    "            lbp_descriptor.append(lbp_obj1.bins)\n",
    "#     print(b)        \n",
    "    return lbp_descriptor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbp_output = LBP_feature(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(lbp_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOG_NeuralNetwork:\n",
    "    def __init__(self, human_feature_vectors, non_human_feature_vectors, epochs = 3, alpha = 0.01, input_layer_neurons = 7524, hidden_layer_neurons = 200, output_layer_neurons = 1):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.input_layer_neurons = input_layer_neurons\n",
    "        self.hidden_layer_neurons = hidden_layer_neurons\n",
    "        self.output_layer_neurons = output_layer_neurons\n",
    "        \n",
    "#         self.output_hidden_layer = np.full((2, 1), 0.0)\n",
    "#         self.pred_output = np.full((, 1), 0.0)\n",
    "#         self.err = np.full((2, 1), 0.0)\n",
    "        \n",
    "        #assigning random weights\n",
    "#         self.hidden_layer_weights = np.random.randn(-0.5, 0.5, size = (self.input_layer_neurons,self.hidden_layer_neurons))\n",
    "#         self.output_layer_weights = np.random.randn(-0.5, 0.5, size = (self.hidden_layer_neurons,self.output_layer_neurons))\n",
    "        self.hidden_layer_weights =  (np.random.random_sample((self.input_layer_neurons,self.hidden_layer_neurons)) -0.5) * 2 * self.alpha\n",
    "        self.output_layer_weights =  (np.random.random_sample((self.hidden_layer_neurons,self.output_layer_neurons)) - 0.5) * 2 * self.alpha\n",
    "    \n",
    "        #assigning random bias       \n",
    "        self.hidden_layer_bias = np.full((1,self.hidden_layer_neurons), -1.0, dtype = float)\n",
    "        self.output_layer_bias = np.full((1,self.output_layer_neurons), -1.0, dtype = float)\n",
    "        \n",
    "#         self.human_feature_vectors = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "        self.human_feature_vectors = human_feature_vectors\n",
    "        self.non_human_feature_vectors = non_human_feature_vectors\n",
    "        \n",
    "        self.output_hidden_layer = None\n",
    "        self.pred_output = None\n",
    "        self.pred_output_delta = None\n",
    "        self.hidden_layer_output_delta = None\n",
    "        \n",
    "        \n",
    "        print(self.non_human_feature_vectors.shape)\n",
    "        self.train_nn()\n",
    "        \n",
    "    # Sigmoid function for the output neuron \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "        \n",
    "    # Derivative of Sigmoid in terms of itself:\n",
    "    def derivative_of_sigmoid(self, x):\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    # ReLu function for the hidden neurons\n",
    "    def relu(self, x):\n",
    "        y = x.copy()\n",
    "        y[x<0] = 0\n",
    "        return y\n",
    "        \n",
    "    # Derivative of ReLU in terms of itself is:    \n",
    "    def derivative_of_relu(self, x):\n",
    "        y = x.copy()\n",
    "        y[x>0] = 1\n",
    "        y[x<=0] = 0\n",
    "        return y\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "#         h_layer_a = np.dot(self.inputs,self.hidden_layer_weights) + self.hidden_layer_bias\n",
    "        self.output_hidden_layer = self.relu(np.dot(self.inputs,self.hidden_layer_weights) + self.hidden_layer_bias)\n",
    "\n",
    "#         o_layer_a = np.dot(self.output_hidden_layer, self.output_layer_weights) + self.output_layer_bias\n",
    "        \n",
    "        self.pred_output = self.sigmoid(np.dot(self.output_hidden_layer, self.output_layer_weights) + self.output_layer_bias)\n",
    "        \n",
    "    def backpropogation(self, target_output):\n",
    "        \n",
    "        final_error = target_output - self.pred_output\n",
    "#         print('final error', final_error)\n",
    "        self.err = np.absolute(target_output - self.pred_output).sum()\n",
    "        self.pred_output_delta = final_error * self.derivative_of_sigmoid(self.pred_output)\n",
    "        \n",
    "        h_layer_error = self.pred_output_delta.dot(self.output_layer_weights.T)\n",
    "        self.hidden_layer_output_delta = h_layer_error * self.derivative_of_relu(self.output_hidden_layer)\n",
    "            \n",
    "    def update(self, inputs):\n",
    "            \n",
    "        #Updating weights and bias\n",
    "#         self.inputs = inputs\n",
    "        self.output_layer_weights = self.output_layer_weights + self.output_hidden_layer.T.dot(self.pred_output_delta) * self.alpha\n",
    "        self.output_layer_bias = self.output_layer_bias + np.sum(self.pred_output_delta,axis = 1, keepdims=True) * self.alpha\n",
    "    \n",
    "        self.hidden_layer_weights = self.hidden_layer_weights + inputs.T.dot(self.hidden_layer_output_delta) * self.alpha\n",
    "        self.hidden_layer_bias = self.hidden_layer_bias + np.sum(self.hidden_layer_output_delta, axis = 1, keepdims=True) * self.alpha\n",
    "\n",
    "        \n",
    "    def train_nn(self):\n",
    "        \n",
    "        avg_error = 0\n",
    "        for _ in range(self.epochs):\n",
    "            target_output = np.array([[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0]]) \n",
    "            self.feedforward(self.human_feature_vectors)\n",
    "            self.backpropogation(target_output)\n",
    "            self.update(self.human_feature_vectors)\n",
    "#             if(np.absolute(avg_error - self.err)<= 0.1):\n",
    "#                 break\n",
    "            avg_error += self.err\n",
    "#             if(avg_error <= 0.1):\n",
    "#                 break\n",
    "        print('Predicted output after training Negative Images:\\n', self.pred_output)\n",
    "        print('Average Error:\\n', avg_error/len(self.human_feature_vectors))\n",
    "        \n",
    "        #save anything from training positive here         \n",
    "        \n",
    "        # reset values for negative features\n",
    "\n",
    "#         #assigning random weights\n",
    "#         self.hidden_layer_weights = np.random.uniform(-0.5, 0.5, size = (self.input_layer_neurons,self.hidden_layer_neurons))\n",
    "#         self.output_layer_weights = np.random.uniform(-0.5, 0.5, size = (self.hidden_layer_neurons,self.output_layer_neurons))\n",
    "\n",
    "#         #assigning random bias       \n",
    "#         self.hidden_layer_bias = np.random.uniform(-1, -1, size = (1,self.hidden_layer_neurons))\n",
    "#         self.output_layer_bias = np.random.uniform(-1, -1, size = (1,self.output_layer_neurons))\n",
    "\n",
    "#         self.pred_output = 0\n",
    "        \n",
    "# #         self.pred_output = np.full((2, 1), 0.0)\n",
    "        \n",
    "#         avg_error = 0\n",
    "#         for _ in range(self.epochs):\n",
    "#             target_output = np.full((10,1),0.0)\n",
    "#             self.feedforward(self.non_human_feature_vectors)\n",
    "#             self.backpropogation(target_output)\n",
    "#             self.update(self.non_human_feature_vectors)\n",
    "#             avg_error += self.err\n",
    "        \n",
    "#         print('Predicted output after training Negative Images:\\n', self.pred_output)\n",
    "#         print('Average Error:\\n', avg_error/len(self.non_human_feature_vectors))\n",
    "        \n",
    "# #         save anything from training negative here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000053a_cut.bmp\n",
      "160\n",
      "00000057a_cut.bmp\n",
      "160\n",
      "00000062a_cut.bmp\n",
      "160\n",
      "00000091a_cut.bmp\n",
      "160\n",
      "00000093a_cut.bmp\n",
      "160\n",
      "01-03e_cut.bmp\n",
      "160\n",
      "no_person__no_bike_213_cut.bmp\n",
      "160\n",
      "no_person__no_bike_219_cut.bmp\n",
      "160\n",
      "no_person__no_bike_247_cut.bmp\n",
      "160\n",
      "no_person__no_bike_259_cut.bmp\n",
      "160\n",
      "Final Output Layer Weights:\n",
      " [[ 1.08891185e-02]\n",
      " [-1.50356528e-02]\n",
      " [-3.16615972e-02]\n",
      " [ 3.28884439e-02]\n",
      " [ 5.70881015e-03]\n",
      " [-2.16454658e-02]\n",
      " [-3.22580430e-02]\n",
      " [ 2.31398561e-02]\n",
      " [ 6.90289088e-03]\n",
      " [ 4.26342034e-02]\n",
      " [ 7.89673801e-03]\n",
      " [ 2.82516963e-02]\n",
      " [-1.76437150e-02]\n",
      " [ 4.39865184e-02]\n",
      " [ 1.83079817e-02]\n",
      " [ 3.36820641e-02]\n",
      " [-2.16468277e-02]\n",
      " [ 3.49824790e-03]\n",
      " [ 4.63012831e-02]\n",
      " [-4.56504497e-02]\n",
      " [ 1.57470286e-02]\n",
      " [-3.39289899e-02]\n",
      " [-2.62134245e-02]\n",
      " [ 1.82126798e-02]\n",
      " [-3.50071681e-02]\n",
      " [ 3.97863942e-02]\n",
      " [-3.99643468e-02]\n",
      " [ 2.20576101e-02]\n",
      " [ 4.82153829e-02]\n",
      " [ 4.45396385e-02]\n",
      " [ 2.20287404e-02]\n",
      " [ 8.08312741e-03]\n",
      " [-5.94736289e-03]\n",
      " [ 6.90072914e-03]\n",
      " [-3.82203011e-02]\n",
      " [ 3.06405269e-02]\n",
      " [-7.29703802e-05]\n",
      " [-1.89694207e-03]\n",
      " [ 5.29616821e-03]\n",
      " [ 1.77049050e-02]\n",
      " [ 3.40147400e-02]\n",
      " [-4.28885389e-02]\n",
      " [ 3.47732934e-02]\n",
      " [ 2.29676555e-02]\n",
      " [ 1.18385842e-03]\n",
      " [-3.95005583e-02]\n",
      " [ 2.81880428e-02]\n",
      " [ 1.58820120e-02]\n",
      " [-4.78611542e-02]\n",
      " [-1.87833957e-02]\n",
      " [-8.40169368e-04]\n",
      " [-1.87754930e-02]\n",
      " [ 1.12452243e-02]\n",
      " [-2.96740216e-02]\n",
      " [-6.30319530e-03]\n",
      " [ 3.03041630e-02]\n",
      " [-3.09940133e-02]\n",
      " [ 2.09124523e-02]\n",
      " [-4.63272379e-02]\n",
      " [ 3.90594624e-02]\n",
      " [-2.94823784e-02]\n",
      " [-2.94371929e-02]\n",
      " [ 1.68601258e-02]\n",
      " [ 1.83990409e-02]\n",
      " [-1.68375486e-03]\n",
      " [-4.66135562e-02]\n",
      " [ 3.06095948e-02]\n",
      " [-3.11371250e-02]\n",
      " [ 4.83403065e-02]\n",
      " [-1.53401143e-02]\n",
      " [-2.75623609e-02]\n",
      " [ 2.88754202e-02]\n",
      " [ 1.74376888e-02]\n",
      " [-4.99679371e-02]\n",
      " [-4.51998592e-02]\n",
      " [-1.88921139e-02]\n",
      " [ 2.39588053e-02]\n",
      " [-3.21182017e-03]\n",
      " [ 3.18382073e-03]\n",
      " [-3.41850594e-02]\n",
      " [-3.88924267e-02]\n",
      " [-9.20419129e-03]\n",
      " [-3.61838497e-02]\n",
      " [-1.19573220e-03]\n",
      " [ 2.44208704e-02]\n",
      " [-9.60136354e-03]\n",
      " [ 4.59647543e-02]\n",
      " [-4.60863327e-02]\n",
      " [-2.68118274e-02]\n",
      " [ 2.16721090e-02]\n",
      " [-1.98133171e-02]\n",
      " [-4.10088275e-02]\n",
      " [ 1.39809016e-02]\n",
      " [ 3.15234528e-02]\n",
      " [-2.93342066e-02]\n",
      " [ 6.29049844e-03]\n",
      " [-3.46964593e-03]\n",
      " [ 7.52562560e-03]\n",
      " [ 1.30262178e-02]\n",
      " [-2.53784651e-02]\n",
      " [ 2.15852044e-02]\n",
      " [-2.98668193e-03]\n",
      " [-2.47177114e-02]\n",
      " [-3.86273230e-02]\n",
      " [ 2.56540215e-02]\n",
      " [ 3.30897100e-02]\n",
      " [ 2.48154208e-02]\n",
      " [-3.72550526e-02]\n",
      " [ 5.30989922e-03]\n",
      " [ 4.56550890e-02]\n",
      " [-2.61899924e-02]\n",
      " [ 3.80908396e-02]\n",
      " [ 1.71659559e-02]\n",
      " [-1.08047819e-02]\n",
      " [ 1.05336013e-02]\n",
      " [ 4.54587587e-02]\n",
      " [-1.64811374e-02]\n",
      " [-4.44849573e-02]\n",
      " [-3.33468197e-02]\n",
      " [-3.22156251e-02]\n",
      " [-3.72002354e-02]\n",
      " [-1.50666180e-02]\n",
      " [ 4.49997102e-02]\n",
      " [ 4.43954766e-02]\n",
      " [ 4.67467655e-02]\n",
      " [-4.26212047e-02]\n",
      " [ 1.34181596e-02]\n",
      " [-2.40162148e-02]\n",
      " [ 2.23785787e-02]\n",
      " [-5.06580811e-03]\n",
      " [-3.22562745e-03]\n",
      " [-2.98779963e-02]\n",
      " [ 3.98153539e-02]\n",
      " [-3.47451229e-02]\n",
      " [ 2.11798238e-02]\n",
      " [ 3.28041407e-02]\n",
      " [ 9.79536084e-03]\n",
      " [-1.13685595e-02]\n",
      " [ 5.43530423e-03]\n",
      " [ 4.14633047e-02]\n",
      " [ 1.13973450e-02]\n",
      " [-8.17221341e-04]\n",
      " [-4.51057813e-02]\n",
      " [ 2.44539926e-02]\n",
      " [-4.67626490e-02]\n",
      " [ 4.79877555e-02]\n",
      " [-2.61937706e-02]\n",
      " [ 3.53534399e-02]\n",
      " [ 1.77129116e-02]\n",
      " [ 1.16359479e-02]\n",
      " [ 4.97256265e-03]\n",
      " [-2.70820173e-02]\n",
      " [ 3.35985671e-02]\n",
      " [ 3.33190911e-02]\n",
      " [ 7.81514475e-03]\n",
      " [-3.97261609e-02]\n",
      " [ 4.10423165e-02]\n",
      " [-1.33084087e-02]\n",
      " [ 4.03757866e-02]\n",
      " [-4.08373939e-02]\n",
      " [-3.89698178e-02]\n",
      " [ 3.01665262e-02]\n",
      " [-2.79488309e-02]\n",
      " [-9.68267038e-03]\n",
      " [ 1.94535897e-03]\n",
      " [ 1.62828583e-03]\n",
      " [ 4.56890989e-02]\n",
      " [-3.38539459e-02]\n",
      " [ 8.08296367e-03]\n",
      " [-3.39791577e-02]\n",
      " [ 4.22601713e-02]\n",
      " [ 1.23914467e-02]\n",
      " [-3.57563290e-02]\n",
      " [-3.79662981e-02]\n",
      " [-4.63617239e-02]\n",
      " [-2.67634276e-02]\n",
      " [ 2.70098037e-02]\n",
      " [-3.52030877e-02]\n",
      " [-1.12285228e-02]\n",
      " [-2.82774780e-02]\n",
      " [-4.02575309e-02]\n",
      " [-4.78004772e-02]\n",
      " [ 3.69292408e-02]\n",
      " [-3.76649967e-02]\n",
      " [-2.00668778e-02]\n",
      " [-4.79711007e-02]\n",
      " [ 4.63628810e-02]\n",
      " [-1.40484725e-02]\n",
      " [-1.10978359e-01]\n",
      " [ 4.51549789e-02]\n",
      " [ 4.61762371e-02]\n",
      " [ 4.63028296e-02]\n",
      " [ 4.66692665e-02]\n",
      " [-2.71624319e-02]\n",
      " [ 4.12793266e-02]\n",
      " [ 1.54747037e-02]\n",
      " [ 3.00389335e-02]\n",
      " [ 2.13242063e-02]\n",
      " [ 4.14099621e-02]\n",
      " [-2.18641527e-02]]\n",
      "Final Hidden Layer Weights:\n",
      " [[-0.01477677 -0.01052289  0.01969808 ... -0.01119243 -0.03644209\n",
      "  -0.00363769]\n",
      " [-0.03828767 -0.01513618  0.00721155 ...  0.02007863  0.04244171\n",
      "   0.02902004]\n",
      " [ 0.03598655 -0.0227933  -0.04265435 ...  0.02751277 -0.0122982\n",
      "   0.0478022 ]\n",
      " ...\n",
      " [ 0.02815893 -0.04680564  0.02735855 ...  0.01929172  0.02571034\n",
      "   0.04472208]\n",
      " [ 0.01144409  0.00401982  0.03517382 ... -0.04987998  0.00755746\n",
      "  -0.01738103]\n",
      " [ 0.02629617 -0.04840257 -0.02344492 ...  0.04847358 -0.03324734\n",
      "  -0.03652747]]\n",
      "Final Output Layer Bias:\n",
      " [[-1.22424997]]\n",
      "Final Hidden Layer Bias:\n",
      " [[-0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583]]\n",
      "Target Output:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "After Training:\n",
      " [[0.21618766]\n",
      " [0.21248884]\n",
      " [0.21258724]\n",
      " [0.20905256]\n",
      " [0.21527312]\n",
      " [0.21763387]\n",
      " [0.21438316]\n",
      " [0.21649592]\n",
      " [0.21349971]\n",
      " [0.20713073]]\n",
      "No human detected and the predicted output is [0.21618766]\n",
      "No human detected and the predicted output is [0.21248884]\n",
      "No human detected and the predicted output is [0.21258724]\n",
      "No human detected and the predicted output is [0.20905256]\n",
      "No human detected and the predicted output is [0.21527312]\n",
      "No human detected and the predicted output is [0.21763387]\n",
      "No human detected and the predicted output is [0.21438316]\n",
      "No human detected and the predicted output is [0.21649592]\n",
      "No human detected and the predicted output is [0.21349971]\n",
      "No human detected and the predicted output is [0.20713073]\n",
      "Average error =  2.43218887092214\n",
      "00000003a_cut.bmp\n",
      "160\n",
      "00000090a_cut.bmp\n",
      "160\n",
      "00000118a_cut.bmp\n",
      "160\n",
      "no_person__no_bike_258_Cut.bmp\n",
      "160\n",
      "no_person__no_bike_264_cut.bmp\n",
      "160\n",
      "Initial Output Layer Weights:\n",
      " [[ 1.08891185e-02]\n",
      " [-1.50356528e-02]\n",
      " [-3.16615972e-02]\n",
      " [ 3.28884439e-02]\n",
      " [ 5.70881015e-03]\n",
      " [-2.16454658e-02]\n",
      " [-3.22580430e-02]\n",
      " [ 2.31398561e-02]\n",
      " [ 6.90289088e-03]\n",
      " [ 4.26342034e-02]\n",
      " [ 7.89673801e-03]\n",
      " [ 2.82516963e-02]\n",
      " [-1.76437150e-02]\n",
      " [ 4.39865184e-02]\n",
      " [ 1.83079817e-02]\n",
      " [ 3.36820641e-02]\n",
      " [-2.16468277e-02]\n",
      " [ 3.49824790e-03]\n",
      " [ 4.63012831e-02]\n",
      " [-4.56504497e-02]\n",
      " [ 1.57470286e-02]\n",
      " [-3.39289899e-02]\n",
      " [-2.62134245e-02]\n",
      " [ 1.82126798e-02]\n",
      " [-3.50071681e-02]\n",
      " [ 3.97863942e-02]\n",
      " [-3.99643468e-02]\n",
      " [ 2.20576101e-02]\n",
      " [ 4.82153829e-02]\n",
      " [ 4.45396385e-02]\n",
      " [ 2.20287404e-02]\n",
      " [ 8.08312741e-03]\n",
      " [-5.94736289e-03]\n",
      " [ 6.90072914e-03]\n",
      " [-3.82203011e-02]\n",
      " [ 3.06405269e-02]\n",
      " [-7.29703802e-05]\n",
      " [-1.89694207e-03]\n",
      " [ 5.29616821e-03]\n",
      " [ 1.77049050e-02]\n",
      " [ 3.40147400e-02]\n",
      " [-4.28885389e-02]\n",
      " [ 3.47732934e-02]\n",
      " [ 2.29676555e-02]\n",
      " [ 1.18385842e-03]\n",
      " [-3.95005583e-02]\n",
      " [ 2.81880428e-02]\n",
      " [ 1.58820120e-02]\n",
      " [-4.78611542e-02]\n",
      " [-1.87833957e-02]\n",
      " [-8.40169368e-04]\n",
      " [-1.87754930e-02]\n",
      " [ 1.12452243e-02]\n",
      " [-2.96740216e-02]\n",
      " [-6.30319530e-03]\n",
      " [ 3.03041630e-02]\n",
      " [-3.09940133e-02]\n",
      " [ 2.09124523e-02]\n",
      " [-4.63272379e-02]\n",
      " [ 3.90594624e-02]\n",
      " [-2.94823784e-02]\n",
      " [-2.94371929e-02]\n",
      " [ 1.68601258e-02]\n",
      " [ 1.83990409e-02]\n",
      " [-1.68375486e-03]\n",
      " [-4.66135562e-02]\n",
      " [ 3.06095948e-02]\n",
      " [-3.11371250e-02]\n",
      " [ 4.83403065e-02]\n",
      " [-1.53401143e-02]\n",
      " [-2.75623609e-02]\n",
      " [ 2.88754202e-02]\n",
      " [ 1.74376888e-02]\n",
      " [-4.99679371e-02]\n",
      " [-4.51998592e-02]\n",
      " [-1.88921139e-02]\n",
      " [ 2.39588053e-02]\n",
      " [-3.21182017e-03]\n",
      " [ 3.18382073e-03]\n",
      " [-3.41850594e-02]\n",
      " [-3.88924267e-02]\n",
      " [-9.20419129e-03]\n",
      " [-3.61838497e-02]\n",
      " [-1.19573220e-03]\n",
      " [ 2.44208704e-02]\n",
      " [-9.60136354e-03]\n",
      " [ 4.59647543e-02]\n",
      " [-4.60863327e-02]\n",
      " [-2.68118274e-02]\n",
      " [ 2.16721090e-02]\n",
      " [-1.98133171e-02]\n",
      " [-4.10088275e-02]\n",
      " [ 1.39809016e-02]\n",
      " [ 3.15234528e-02]\n",
      " [-2.93342066e-02]\n",
      " [ 6.29049844e-03]\n",
      " [-3.46964593e-03]\n",
      " [ 7.52562560e-03]\n",
      " [ 1.30262178e-02]\n",
      " [-2.53784651e-02]\n",
      " [ 2.15852044e-02]\n",
      " [-2.98668193e-03]\n",
      " [-2.47177114e-02]\n",
      " [-3.86273230e-02]\n",
      " [ 2.56540215e-02]\n",
      " [ 3.30897100e-02]\n",
      " [ 2.48154208e-02]\n",
      " [-3.72550526e-02]\n",
      " [ 5.30989922e-03]\n",
      " [ 4.56550890e-02]\n",
      " [-2.61899924e-02]\n",
      " [ 3.80908396e-02]\n",
      " [ 1.71659559e-02]\n",
      " [-1.08047819e-02]\n",
      " [ 1.05336013e-02]\n",
      " [ 4.54587587e-02]\n",
      " [-1.64811374e-02]\n",
      " [-4.44849573e-02]\n",
      " [-3.33468197e-02]\n",
      " [-3.22156251e-02]\n",
      " [-3.72002354e-02]\n",
      " [-1.50666180e-02]\n",
      " [ 4.49997102e-02]\n",
      " [ 4.43954766e-02]\n",
      " [ 4.67467655e-02]\n",
      " [-4.26212047e-02]\n",
      " [ 1.34181596e-02]\n",
      " [-2.40162148e-02]\n",
      " [ 2.23785787e-02]\n",
      " [-5.06580811e-03]\n",
      " [-3.22562745e-03]\n",
      " [-2.98779963e-02]\n",
      " [ 3.98153539e-02]\n",
      " [-3.47451229e-02]\n",
      " [ 2.11798238e-02]\n",
      " [ 3.28041407e-02]\n",
      " [ 9.79536084e-03]\n",
      " [-1.13685595e-02]\n",
      " [ 5.43530423e-03]\n",
      " [ 4.14633047e-02]\n",
      " [ 1.13973450e-02]\n",
      " [-8.17221341e-04]\n",
      " [-4.51057813e-02]\n",
      " [ 2.44539926e-02]\n",
      " [-4.67626490e-02]\n",
      " [ 4.79877555e-02]\n",
      " [-2.61937706e-02]\n",
      " [ 3.53534399e-02]\n",
      " [ 1.77129116e-02]\n",
      " [ 1.16359479e-02]\n",
      " [ 4.97256265e-03]\n",
      " [-2.70820173e-02]\n",
      " [ 3.35985671e-02]\n",
      " [ 3.33190911e-02]\n",
      " [ 7.81514475e-03]\n",
      " [-3.97261609e-02]\n",
      " [ 4.10423165e-02]\n",
      " [-1.33084087e-02]\n",
      " [ 4.03757866e-02]\n",
      " [-4.08373939e-02]\n",
      " [-3.89698178e-02]\n",
      " [ 3.01665262e-02]\n",
      " [-2.79488309e-02]\n",
      " [-9.68267038e-03]\n",
      " [ 1.94535897e-03]\n",
      " [ 1.62828583e-03]\n",
      " [ 4.56890989e-02]\n",
      " [-3.38539459e-02]\n",
      " [ 8.08296367e-03]\n",
      " [-3.39791577e-02]\n",
      " [ 4.22601713e-02]\n",
      " [ 1.23914467e-02]\n",
      " [-3.57563290e-02]\n",
      " [-3.79662981e-02]\n",
      " [-4.63617239e-02]\n",
      " [-2.67634276e-02]\n",
      " [ 2.70098037e-02]\n",
      " [-3.52030877e-02]\n",
      " [-1.12285228e-02]\n",
      " [-2.82774780e-02]\n",
      " [-4.02575309e-02]\n",
      " [-4.78004772e-02]\n",
      " [ 3.69292408e-02]\n",
      " [-3.76649967e-02]\n",
      " [-2.00668778e-02]\n",
      " [-4.79711007e-02]\n",
      " [ 4.63628810e-02]\n",
      " [-1.40484725e-02]\n",
      " [-1.10978359e-01]\n",
      " [ 4.51549789e-02]\n",
      " [ 4.61762371e-02]\n",
      " [ 4.63028296e-02]\n",
      " [ 4.66692665e-02]\n",
      " [-2.71624319e-02]\n",
      " [ 4.12793266e-02]\n",
      " [ 1.54747037e-02]\n",
      " [ 3.00389335e-02]\n",
      " [ 2.13242063e-02]\n",
      " [ 4.14099621e-02]\n",
      " [-2.18641527e-02]]\n",
      "Initial Hidden Layer Weights:\n",
      " [[-0.01477677 -0.01052289  0.01969808 ... -0.01119243 -0.03644209\n",
      "  -0.00363769]\n",
      " [-0.03828767 -0.01513618  0.00721155 ...  0.02007863  0.04244171\n",
      "   0.02902004]\n",
      " [ 0.03598655 -0.0227933  -0.04265435 ...  0.02751277 -0.0122982\n",
      "   0.0478022 ]\n",
      " ...\n",
      " [ 0.02815893 -0.04680564  0.02735855 ...  0.01929172  0.02571034\n",
      "   0.04472208]\n",
      " [ 0.01144409  0.00401982  0.03517382 ... -0.04987998  0.00755746\n",
      "  -0.01738103]\n",
      " [ 0.02629617 -0.04840257 -0.02344492 ...  0.04847358 -0.03324734\n",
      "  -0.03652747]]\n",
      "Initial Output Layer Bias:\n",
      " [[-1.22424997]]\n",
      "Initial Hidden Layer Bias:\n",
      " [[-0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583 -0.98537583\n",
      "  -0.98537583 -0.98537583]]\n",
      "Target Output:\n",
      " [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "After Training:\n",
      " [[0.20951342]\n",
      " [0.2082819 ]\n",
      " [0.20326052]\n",
      " [0.21353309]\n",
      " [0.20767411]]\n",
      "No human detected and the predicted output is [0.20951342]\n",
      "No human detected and the predicted output is [0.2082819]\n",
      "No human detected and the predicted output is [0.20326052]\n",
      "No human detected and the predicted output is [0.21353309]\n",
      "No human detected and the predicted output is [0.20767411]\n"
     ]
    }
   ],
   "source": [
    "class Driver:\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        \n",
    "        train_path_positive = \"C://Users//anvit//OneDrive//Desktop//HoG//Training_images(Pos)\"\n",
    "        train_path_negative = \"C://Users//anvit//OneDrive//Desktop//HoG//Training_images(Neg)\"\n",
    "        test_neg = \"C://Users//anvit//OneDrive//Desktop//HoG//Test_images(Neg)\"\n",
    "        test_pos = \"C://Users//anvit//OneDrive//Desktop//HoG//Test_images(Pos)\"\n",
    "        \n",
    "        def gen_hog_vectors(path):\n",
    "            vectors = []\n",
    "            files = os.listdir(path)\n",
    "            for file in files:\n",
    "                print(file)\n",
    "                t = Preprocessing(path + '//' + file)\n",
    "                image = t.img_array\n",
    "#         print(image)\n",
    "                gray = t.gray_array\n",
    "#         print(gray)\n",
    "                g_x = t.g_x\n",
    "#         print(g_x[28][28])\n",
    "                g_y = t.g_y\n",
    "#         print(g_y[28][28])\n",
    "                g_mag = t.g_mag\n",
    "#         print(g_mag[28][28])\n",
    "                theta = t.theta\n",
    "#         print(theta[28][28])\n",
    "                h_feature_output = np.array(h_feature(g_mag,theta))\n",
    "#                 print(h_feature_output.shape)\n",
    "#                 h_feature_output = h_feature(g_mag,theta)\n",
    "                vectors.append(h_feature_output)\n",
    "                result = np.array(vectors)\n",
    "#                 print(result)\n",
    "            return result\n",
    "        \n",
    "        def train_nn(inputs, target_output):\n",
    "            \n",
    "            epochs = 10\n",
    "            alpha = 0.05\n",
    "            input_layer_neurons = 7524\n",
    "            hidden_layer_neurons = 200\n",
    "            output_layer_neurons = 1\n",
    "            err = 0\n",
    "            \n",
    "#             inputs = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "#             target_output = np.array([[0],[1],[1],[0]]) \n",
    "            \n",
    "            # Step 1: Initialize the weights and biases to have random values between 0.5 and 0.5:\n",
    "#             weights\n",
    "            hidden_layer_weights = np.random.uniform(-0.5,0.5, size = (input_layer_neurons,hidden_layer_neurons)) * 2 * alpha\n",
    "            output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons)) * 2 * alpha\n",
    "\n",
    "            # bias:\n",
    "            hidden_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,hidden_layer_neurons))\n",
    "            output_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,output_layer_neurons))\n",
    "\n",
    "#             print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "#             print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "#             print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "#             print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "                \n",
    "            # Step 2: Training the inputs\n",
    "\n",
    "            # Step 2a:\n",
    "            # Setting the corresponding activation(a1) for input_layer\n",
    "            def sigmoid(x):\n",
    "                return 1/(1 + np.exp(-x))\n",
    "\n",
    "            # We need the derivative of this for further calculations\n",
    "            def derivative_of_sigmoid(x):\n",
    "            #     derivative of sigmoid in terms of itslef is:\n",
    "                return x * (1 - x)\n",
    "            \n",
    "            def relu(x):\n",
    "                y = x.copy()\n",
    "                y[x<0] = 0\n",
    "                return y\n",
    "        \n",
    "            # Derivative of ReLU in terms of itself is:    \n",
    "            def derivative_of_relu(x):\n",
    "                y = x.copy()\n",
    "                y[x>0] = 1\n",
    "                y[x<=0] = 0\n",
    "                return y\n",
    "\n",
    "            for i in range(epochs):\n",
    "                #     Step 2b:\n",
    "                #     Feedforward\n",
    "                h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "                h_layer_a = h_layer_a + hidden_layer_bias\n",
    "                output_hidden_layer = relu(h_layer_a)\n",
    "    \n",
    "                o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "                o_layer_a = o_layer_a + output_layer_bias\n",
    "                pred_output = sigmoid(o_layer_a)\n",
    "\n",
    "                #     Step 2c:\n",
    "                #     Output error\n",
    "                \n",
    "                final_error = target_output - pred_output\n",
    "                err = err + np.absolute(target_output - pred_output).sum()\n",
    "                pred_output_delta = final_error * derivative_of_sigmoid(pred_output)\n",
    "\n",
    "                #     Step 2d:\n",
    "                #     Backpropogating the error\n",
    "                h_layer_error = pred_output_delta.dot(output_layer_weights.T)\n",
    "                hidden_layer_output_delta = h_layer_error * derivative_of_relu(output_hidden_layer)\n",
    "\n",
    "                #     Step 2e:\n",
    "                #     Updating weights and bias\n",
    "                output_layer_weights = output_layer_weights + output_hidden_layer.T.dot(pred_output_delta) * alpha\n",
    "                output_layer_bias = output_layer_bias + np.sum(pred_output_delta) * alpha\n",
    "    \n",
    "                hidden_layer_weights = hidden_layer_weights + inputs.T.dot(hidden_layer_output_delta) * alpha\n",
    "                hidden_layer_bias = hidden_layer_bias + np.sum(hidden_layer_output_delta) * alpha\n",
    "\n",
    "            print('Final Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Final Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Final Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Final Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "\n",
    "            print('Target Output:\\n', target_output)\n",
    "            print('After Training:\\n', pred_output)\n",
    "            for i in range(pred_output.shape[0]):\n",
    "                if(pred_output[i] >= 0.6):\n",
    "                    print('Human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] > 0.4 and pred_output[i] < 0.6):\n",
    "                    print('Boarderline human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] <= 0.4):\n",
    "                    print('No human detected and the predicted output is', pred_output[i])\n",
    "            print('Average error = ', err/ (inputs.shape[0]))\n",
    "            return hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias\n",
    "                    \n",
    "        def test_nn(inputs,target_output,hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias):\n",
    "            \n",
    "            epochs = 10\n",
    "            alpha = 0.05\n",
    "            input_layer_neurons = 7524\n",
    "            hidden_layer_neurons = 200\n",
    "            output_layer_neurons = 1\n",
    "            err = 0\n",
    "            \n",
    "#             inputs = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "#             target_output = np.array([[0],[1],[1],[0]]) \n",
    "            \n",
    "            # Step 1: Initialize the weights and biases to have random values between 0.5 and 0.5:\n",
    "            # weights\n",
    "#             hidden_layer_weights = np.random.uniform(-0.5,0.5, size = (input_layer_neurons,hidden_layer_neurons)) * 2 * 0.01\n",
    "#             output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons)) * 2 * 0.01\n",
    "\n",
    "#             # bias:\n",
    "#             hidden_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,hidden_layer_neurons))\n",
    "#             output_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,output_layer_neurons))\n",
    "\n",
    "            print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "                \n",
    "            # Step 2: Training the inputs\n",
    "\n",
    "            # Step 2a:\n",
    "            # Setting the corresponding activation(a1) for input_layer\n",
    "            def sigmoid(x):\n",
    "                return 1/(1 + np.exp(-x))\n",
    "\n",
    "            # We need the derivative of this for further calculations\n",
    "            def derivative_of_sigmoid(x):\n",
    "            #     derivative of sigmoid in terms of itslef is:\n",
    "                return x * (1 - x)\n",
    "            \n",
    "            def relu(x):\n",
    "                y = x.copy()\n",
    "                y[x<0] = 0\n",
    "                return y\n",
    "        \n",
    "            # Derivative of ReLU in terms of itself is:    \n",
    "            def derivative_of_relu(x):\n",
    "                y = x.copy()\n",
    "                y[x>0] = 1\n",
    "                y[x<=0] = 0\n",
    "                return y\n",
    "\n",
    "#             for i in range(epochs):\n",
    "                #     Step 2b:\n",
    "                #     Feedforward\n",
    "            h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "            h_layer_a = h_layer_a + hidden_layer_bias\n",
    "            output_hidden_layer = relu(h_layer_a)\n",
    "    \n",
    "            o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "            o_layer_a = o_layer_a + output_layer_bias\n",
    "            pred_output = sigmoid(o_layer_a)\n",
    "            \n",
    "            print('Target Output:\\n', target_output)\n",
    "            print('After Training:\\n', pred_output)\n",
    "            for i in range(pred_output.shape[0]):\n",
    "                if(pred_output[i][0] >= 0.6):\n",
    "                    print('Human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i][0] > 0.4 and pred_output[i][0] < 0.6):\n",
    "                    print('Boarderline human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i][0] <= 0.4):\n",
    "                    print('No human detected and the predicted output is', pred_output[i])\n",
    "#             print('Average error = ', err/ (inputs.shape[0]))\n",
    "        \n",
    "#         positive_hog_feature_vectors = gen_hog_vectors(train_path_positive)\n",
    "#         target_output_pos = np.full((10,1), 1.0, dtype = float)\n",
    "#         hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias = train_nn(positive_hog_feature_vectors, target_output_pos)\n",
    "        \n",
    "#         positive_test_features = gen_hog_vectors(test_pos)\n",
    "#         target_output_test_pos = np.full((5,1), 1.0, dtype = float)\n",
    "#         pos_test = test_nn(positive_test_features, target_output_test_pos, hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias)\n",
    "        \n",
    "        negative_hog_feature_vectors = gen_hog_vectors(train_path_negative)\n",
    "        target_output_neg = np.full((10,1), 0.0, dtype = float)\n",
    "        hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias = train_nn(negative_hog_feature_vectors, target_output_neg)\n",
    "\n",
    "        negative_test_features = gen_hog_vectors(test_neg)\n",
    "        target_output_test_neg = np.full((5,1), 0.0, dtype = float)\n",
    "        neg_test = test_nn(negative_test_features, target_output_test_neg, hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00126032, 0.0082592 ],\n",
       "       [0.00473213, 0.00837586],\n",
       "       [0.00664615, 0.00044849]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,2)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49141856,  0.31558122],\n",
       "       [-0.28049982, -0.41756153],\n",
       "       [ 0.34165949, -0.33732752]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (0.5 - (-0.5))* np.random.random_sample((3, 2)) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
