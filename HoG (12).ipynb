{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "from itertools import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \n",
    "        self.img = np.full((160, 96), 0)\n",
    "        self.img_array = np.full((160, 96), 0)\n",
    "        self.gray = np.full((160, 96), 0)\n",
    "        self.gray_array = np.full((160, 96), 0)\n",
    "        \n",
    "        self.G_X = np.full((160, 96), 0)\n",
    "        self.G_Y = np.full((160, 96), 0)\n",
    "        self.g_x = np.full((160, 96), 0)\n",
    "        self.g_y = np.full((160, 96), 0)\n",
    "        self.g_mag = np.full((160, 96), 0)\n",
    "        self.sobel_image = np.full((160, 96), 0)\n",
    "        \n",
    "        self.theta = np.full((160, 96), 0)       \n",
    "        \n",
    "        self.path = path\n",
    "        self.img_array = np.full((160, 96), 0)\n",
    "        self.image_read()\n",
    "        self.gray = self.rgb2gray(self.img_array)\n",
    "        self.image_write(self.gray)\n",
    "        \n",
    "        self.g_x, self.g_y, self.sobel_output = np.round(self.sobels_operator(self.gray))\n",
    "        self.theta = self.gradient_angles(self.gray_array, self.g_x, self.g_y)\n",
    "#         print(self.g_x[28][28])\n",
    "#         print(self.g_y[28][28])\n",
    "#         print(self.theta[28][28])\n",
    "        \n",
    "#         return  self.img_array, self.gray_array, self.g_mag, self.theta\n",
    "        \n",
    "#         self\n",
    "    \n",
    "    def image_read(self):\n",
    "        self.img = cv2.imread(self.path)  \n",
    "        self.img_array = np.array(self.img, dtype=float)\n",
    "#         self.gray = self.rgb2gray(self.img)\n",
    "#         self.\n",
    "    \n",
    "    def rgb2gray(self, rgb):\n",
    "        return np.round(np.dot(rgb[...,:3], [0.299, 0.587, 0.114]))\n",
    "    \n",
    "    def image_write(self, gray):\n",
    "        self.img_gray = cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray.bmp', self.gray)\n",
    "        self.gray_array = np.array(gray, dtype=float)\n",
    "#         return self.gray_array\n",
    "\n",
    "    #sobels operator\n",
    "    def sobels_operator(self, inp):\n",
    "        self.inp = inp\n",
    "        g_x = np.array([[-1,0,1],\n",
    "                       [-2,0,2],\n",
    "                       [-1,0,1]])\n",
    "        g_y = np.array([[1,2,1],\n",
    "                       [0,0,0],\n",
    "                       [-1,-2,-1]])\n",
    "    \n",
    "        #Horizontal Gradient \n",
    "        self.G_X = self.conv_sobel(self.inp,g_x)\n",
    "        #Saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobelGX.bmp', self.G_X)\n",
    "       \n",
    "        #Vertical Gradient\n",
    "        self.G_Y = self.conv_sobel(self.inp,g_y)\n",
    "        #saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobelGY.bmp', self.G_Y)\n",
    "    \n",
    "        #Gradient Magnitude\n",
    "        self.g_mag = np.sqrt(np.square(self.G_X) + np.square(self.G_Y))\n",
    "        #saving the image\n",
    "        cv2.imwrite('C:/Users/anvit/OneDrive/Desktop/HoG/Test_results/test_gray_sobel.bmp', self.g_mag)    \n",
    "        return self.G_X, self.G_Y, self.g_mag\n",
    "\n",
    "    #convolution done by implementing using 2D directly\n",
    "    def conv_sobel(self,x,y):\n",
    "        \n",
    "        row_size = x.shape[0]\n",
    "        column_size = x.shape[1]        \n",
    "        conv_img = np.zeros((row_size, column_size), dtype=np.float)\n",
    "        \n",
    "        for a in range(0, row_size - 2):\n",
    "            for b in range(0, column_size - 2):\n",
    "                \n",
    "                conv_img[a + 1][b + 1] = (np.sum(x[a: a + 3, b: b + 3] * y)) / 4\n",
    "\n",
    "        return conv_img\n",
    "    \n",
    "    def gradient_angles(self, gray_array, g_x, g_y):\n",
    "        #Finding the Gradient Angles i.e., theta\n",
    "        print(self.gray_array.shape[0])\n",
    "        #arctan2 is tan inverse, which gives us results in radians with [-pi to pi]\n",
    "        self.theta = np.zeros((self.gray_array.shape[0],self.gray_array.shape[1]))\n",
    "        for i in range(self.gray_array.shape[0]):\n",
    "            for j in range(0, self.gray_array.shape[1]):\n",
    "                if(self.g_y[i][j] == 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = 0\n",
    "                elif(self.g_y[i][j] > 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = 90\n",
    "                elif(self.g_y[i][j] < 0 and self.g_x[i][j] == 0):\n",
    "                    self.theta[i][j] = -90\n",
    "                elif(self.g_y[i][j] == 0):\n",
    "                    self.theta[i][j] = 0\n",
    "                else:\n",
    "                    if(self.g_x[i][j] == 0):\n",
    "                        print('here')\n",
    "                    self.theta[i][j] = np.arctan2(self.g_y[i][j],self.g_x[i][j])\n",
    "    \n",
    "        #Converting radians to degrees\n",
    "        self.theta = np.rad2deg(self.theta)\n",
    "        #converting all the negatives into positives by adding 360 so the range is now [0 to 360]\n",
    "        for i in range(0, self.gray_array.shape[0]):\n",
    "            for j in range(0, self.gray_array.shape[1]):\n",
    "                if(self.theta[i][j] < 0):\n",
    "                    self.theta[i][j] += 360\n",
    "                    \n",
    "        return self.theta        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Histogram:\n",
    "    def __init__(self, mag_block, theta_block):\n",
    "        self.center = np.array([0,20,40,60,80,100,120,140,160])\n",
    "#         4 histograms of 9 bins each - per block\n",
    "        self.bins = np.zeros((4,9), dtype=float)\n",
    "        self.cell_size = 8\n",
    "        self.cells = []\n",
    "        self.t_cells = []\n",
    "        self.m_cells = []\n",
    "        self.flattened = []\n",
    "        self.theta_block = theta_block\n",
    "        self.mag_block = mag_block\n",
    "        self.convert_block_2_cell(mag_block,theta_block)\n",
    "#         c=0\n",
    "#         print(c)\n",
    "        self.generate_features(self.m_cells, self.t_cells)\n",
    "        self.norm_hog()\n",
    "        self.flattened_bins()\n",
    "        \n",
    "    def flattened_bins(self):\n",
    "        for bin in self.bins:\n",
    "            for item in bin:\n",
    "                self.flattened.append(item)\n",
    "        \n",
    "    \n",
    "    def norm_hog(self):\n",
    "        \n",
    "        summation = 0\n",
    "        for bin in self.bins:\n",
    "            for j in range(len(bin)):\n",
    "                summation += bin[j] ** 2\n",
    "        \n",
    "        dist = summation ** 0.5\n",
    "                   \n",
    "        for bin in self.bins:\n",
    "            for j in range(len(bin)):\n",
    "                if(dist == 0):\n",
    "                    continue\n",
    "                else:\n",
    "                    bin[j] /= dist\n",
    "\n",
    "    \n",
    "    def convert_block_2_cell(self, mag_block, theta_block):\n",
    "        c=0\n",
    "        for m in range(0,theta_block.shape[0], self.cell_size):\n",
    "                for n in range(0, theta_block.shape[1], self.cell_size):\n",
    "                    \n",
    "                    m_cell = np.zeros((self.cell_size,self.cell_size))\n",
    "                    t_cell = np.zeros((self.cell_size,self.cell_size))\n",
    "                    \n",
    "                    for p in range(self.cell_size):\n",
    "                        for q in range(self.cell_size):\n",
    "                            \n",
    "                            m_cell[p][q] = mag_block[m+p][n+q]\n",
    "                            t_cell[p][q] = theta_block[m+p][n+q]\n",
    "                     \n",
    "                    c+=1\n",
    "                    self.m_cells.append(m_cell)\n",
    "                    self.t_cells.append(t_cell)\n",
    "#         print(c)\n",
    "    \n",
    "    def generate_features(self, m_cells, t_cells):\n",
    "        \n",
    "        for i in range(len(t_cells)):\n",
    "            for j in range(len(t_cells[i])):\n",
    "                for k in range(len(t_cells[i][j])):\n",
    "                    self.cal_hist(m_cells[i][j][k],t_cells[i][j][k], self.bins[i]) \n",
    "                \n",
    "# HANDLE EDGE CASE : -20,10       \n",
    "    def cal_hist(self, mag, angle, bins):       \n",
    "        \n",
    "#         COMPUTE THE DISTANCE TO ADD THE MAG WRT THE CENTER\n",
    "        if(angle >= 180):\n",
    "            angle -= 180\n",
    "#         now we have all unsigned angles\n",
    "# new case to consider: if angle is >=160 first = bin 9 and second is bin 1\n",
    "        \n",
    "        if(angle >= 160):\n",
    "            first_bin = 8\n",
    "            second_bin = 0\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag \n",
    "#         edge care , if angle is >=350 it's between bins 1 and 9\n",
    "        \n",
    "        if(angle >= 0 and angle < 20):\n",
    "            first_bin = 0\n",
    "            second_bin = 1\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 20 and angle < 40):\n",
    "            first_bin = 1\n",
    "            second_bin = 2\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "        if(angle >= 40 and angle < 60):\n",
    "            first_bin = 2\n",
    "            second_bin = 3\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "        if(angle >= 60 and angle < 80):\n",
    "            first_bin = 3\n",
    "            second_bin = 4\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 80 and angle < 100):\n",
    "            first_bin = 4\n",
    "            second_bin = 5\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 100 and angle < 120):\n",
    "            first_bin = 5\n",
    "            second_bin = 6\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "        \n",
    "        if(angle >= 120 and angle < 140):\n",
    "            first_bin = 6\n",
    "            second_bin = 7\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "         \n",
    "        if(angle >= 140 and angle < 160):\n",
    "            first_bin = 7\n",
    "            second_bin = 8\n",
    "            percentage = self.calc_distance(angle, self.center[first_bin])\n",
    "            bins[first_bin] += (1-percentage) * mag\n",
    "            bins[second_bin] += percentage * mag\n",
    "            \n",
    "    def calc_distance(self, angle, center):        \n",
    "        percent = (np.absolute(angle - center))/20\n",
    "        return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_feature(mag, theta): \n",
    "    descriptor = []\n",
    "    cell_size = 8\n",
    "    block_size = 16\n",
    "    block_overlap = 8\n",
    "    assert(mag.shape[0] == theta.shape[0])\n",
    "    assert(mag.shape[1] == theta.shape[1])\n",
    "    b=0\n",
    "    c=0\n",
    "    \n",
    "#     generating blocks from pixels\n",
    "    for i in range(0,theta.shape[0]-cell_size, cell_size):\n",
    "        for j in range(0,theta.shape[1]-cell_size, cell_size):\n",
    "            \n",
    "            m_block = np.zeros((block_size,block_size))\n",
    "            t_block = np.zeros((block_size,block_size))\n",
    "            \n",
    "            for k in range(block_size):\n",
    "                for l in range(block_size):  \n",
    "                    \n",
    "                    m_block[k][l] = mag[i+k][j+l]\n",
    "                    t_block[k][l] = theta[i+k][j+l]\n",
    "            b +=1\n",
    "\n",
    "            hist_obj = Histogram(m_block,t_block)\n",
    "            for item in hist_obj.flattened:\n",
    "                descriptor.append(item)\n",
    "#     print(b)\n",
    "#     print(c)\n",
    "    return descriptor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_feature_output = np.array(h_feature(sobel_output,theta))\n",
    "# print(np.concatenate(h_feature_output).ravel().tolist())\n",
    "# print(h_feature_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LBP:\n",
    "    def __init__(self, gray_block):\n",
    "        self.mag = gray_block\n",
    "        self.bins = np.zeros((59,), dtype=float)\n",
    "        self.generate_features(gray_block)\n",
    "        self.norm_lbp()\n",
    "        \n",
    "    def norm_lbp(self):\n",
    "        for i in range(len(self.bins)):\n",
    "            self.bins[i] = self.bins[i] / 256\n",
    "    \n",
    "    def generate_features(self,gray_block):\n",
    "        for i in range(gray_block.shape[0]):\n",
    "            for j in range(gray_block.shape[1]):\n",
    "                self.compute_LBP_pattern(i,j,gray_block)\n",
    "        \n",
    "#     def norm_lbp(self):\n",
    "#         for i in range(len(self.bins)):\n",
    "#             self.bins[i] = self.bins[i] / 256\n",
    "        \n",
    "    def compute_LBP_pattern(self,i, j,gray_block):\n",
    "        x = gray_block\n",
    "        pattern = ''\n",
    "        \n",
    "        try:\n",
    "            if(x[i-1][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i-1][j] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i-1][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            if(x[i+1][j+1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i+1][j] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i+1][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "            \n",
    "            if(x[i][j-1] > x[i][j]):\n",
    "                pattern += '1'\n",
    "            else:\n",
    "                pattern += '0'\n",
    "                \n",
    "            self.append_to_bins(pattern)\n",
    "            \n",
    "        except IndexError:\n",
    "            pattern = '00000101'\n",
    "            self.append_to_bins(pattern)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def append_to_bins(self, lbp_pattern):        \n",
    "        decimal = int(lbp_pattern, 2)\n",
    "        patterns = {\n",
    "            0: 0,\n",
    "            1: 1,\n",
    "            2: 2,\n",
    "            3: 3,\n",
    "            4: 4,\n",
    "            6: 5,\n",
    "            7: 6,\n",
    "            8: 7,\n",
    "            12: 8,\n",
    "            14: 9,\n",
    "            15: 10,\n",
    "            16: 11,\n",
    "            24: 12,\n",
    "            28: 13,\n",
    "            30: 14,\n",
    "            31: 15,\n",
    "            32: 16,\n",
    "            48: 17,\n",
    "            56: 18,\n",
    "            60: 19,\n",
    "            62: 20,\n",
    "            63: 21,\n",
    "            64: 22,\n",
    "            96: 23,\n",
    "            112: 24,\n",
    "            120: 25,\n",
    "            124: 26,\n",
    "            126: 27,\n",
    "            127: 28,\n",
    "            128: 29,\n",
    "            129: 30,\n",
    "            131: 31,\n",
    "            135: 32,\n",
    "            143: 33,\n",
    "            159: 34,\n",
    "            191: 35,\n",
    "            192: 36,\n",
    "            193: 37,\n",
    "            195: 38,\n",
    "            199: 39,\n",
    "            207: 40,\n",
    "            223: 41,\n",
    "            224: 42,\n",
    "            225: 43,\n",
    "            227: 44,\n",
    "            231: 45,\n",
    "            239: 46,\n",
    "            240: 47,\n",
    "            241: 48,\n",
    "            243: 49,\n",
    "            247: 50,\n",
    "            248: 51,\n",
    "            249: 52,\n",
    "            251: 53,\n",
    "            252: 54,\n",
    "            253: 55,\n",
    "            254: 56,\n",
    "            255: 57\n",
    "        }\n",
    "        bin_number = patterns.get(decimal, 58)\n",
    "        self.bins[bin_number] += 1   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LBP_feature(gray):\n",
    "    lbp_descriptor = []\n",
    "    cell_size = 8\n",
    "    block_size = 16\n",
    "    block_overlap = 8\n",
    "    b=0\n",
    "#     c=0\n",
    "    \n",
    "#     generating blocks from pixels   \n",
    "    for i in range(0,gray.shape[0],block_size):\n",
    "        for j in range(0,gray.shape[1],block_size):\n",
    "            \n",
    "            gray_block = np.zeros((block_size,block_size))\n",
    "            \n",
    "            for p in range(block_size):\n",
    "                for q in range(block_size):\n",
    "                    \n",
    "                    gray_block[p][q] = gray[i+p][j+q]\n",
    "            \n",
    "            \n",
    "            b+=1\n",
    "            lbp_obj1 = LBP(gray_block)\n",
    "            lbp_descriptor.append(lbp_obj1.bins)\n",
    "#     print(b)        \n",
    "    return lbp_descriptor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbp_output = LBP_feature(gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(lbp_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOG_NeuralNetwork:\n",
    "    def __init__(self, human_feature_vectors, non_human_feature_vectors, epochs = 3, alpha = 0.01, input_layer_neurons = 7524, hidden_layer_neurons = 200, output_layer_neurons = 1):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.alpha = alpha\n",
    "        self.input_layer_neurons = input_layer_neurons\n",
    "        self.hidden_layer_neurons = hidden_layer_neurons\n",
    "        self.output_layer_neurons = output_layer_neurons\n",
    "        \n",
    "#         self.output_hidden_layer = np.full((2, 1), 0.0)\n",
    "#         self.pred_output = np.full((, 1), 0.0)\n",
    "#         self.err = np.full((2, 1), 0.0)\n",
    "        \n",
    "        #assigning random weights\n",
    "#         self.hidden_layer_weights = np.random.randn(-0.5, 0.5, size = (self.input_layer_neurons,self.hidden_layer_neurons))\n",
    "#         self.output_layer_weights = np.random.randn(-0.5, 0.5, size = (self.hidden_layer_neurons,self.output_layer_neurons))\n",
    "        self.hidden_layer_weights =  (np.random.random_sample((self.input_layer_neurons,self.hidden_layer_neurons)) -0.5) * 2 * self.alpha\n",
    "        self.output_layer_weights =  (np.random.random_sample((self.hidden_layer_neurons,self.output_layer_neurons)) - 0.5) * 2 * self.alpha\n",
    "    \n",
    "        #assigning random bias       \n",
    "        self.hidden_layer_bias = np.full((1,self.hidden_layer_neurons), -1.0, dtype = float)\n",
    "        self.output_layer_bias = np.full((1,self.output_layer_neurons), -1.0, dtype = float)\n",
    "        \n",
    "#         self.human_feature_vectors = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "        self.human_feature_vectors = human_feature_vectors\n",
    "        self.non_human_feature_vectors = non_human_feature_vectors\n",
    "        \n",
    "        self.output_hidden_layer = None\n",
    "        self.pred_output = None\n",
    "        self.pred_output_delta = None\n",
    "        self.hidden_layer_output_delta = None\n",
    "        \n",
    "        \n",
    "        print(self.non_human_feature_vectors.shape)\n",
    "        self.train_nn()\n",
    "        \n",
    "    # Sigmoid function for the output neuron \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "        \n",
    "    # Derivative of Sigmoid in terms of itself:\n",
    "    def derivative_of_sigmoid(self, x):\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    # ReLu function for the hidden neurons\n",
    "    def relu(self, x):\n",
    "        y = x.copy()\n",
    "        y[x<0] = 0\n",
    "        return y\n",
    "        \n",
    "    # Derivative of ReLU in terms of itself is:    \n",
    "    def derivative_of_relu(self, x):\n",
    "        y = x.copy()\n",
    "        y[x>0] = 1\n",
    "        y[x<=0] = 0\n",
    "        return y\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        \n",
    "        self.inputs = inputs\n",
    "#         h_layer_a = np.dot(self.inputs,self.hidden_layer_weights) + self.hidden_layer_bias\n",
    "        self.output_hidden_layer = self.relu(np.dot(self.inputs,self.hidden_layer_weights) + self.hidden_layer_bias)\n",
    "\n",
    "#         o_layer_a = np.dot(self.output_hidden_layer, self.output_layer_weights) + self.output_layer_bias\n",
    "        \n",
    "        self.pred_output = self.sigmoid(np.dot(self.output_hidden_layer, self.output_layer_weights) + self.output_layer_bias)\n",
    "        \n",
    "    def backpropogation(self, target_output):\n",
    "        \n",
    "        final_error = target_output - self.pred_output\n",
    "#         print('final error', final_error)\n",
    "        self.err = np.absolute(target_output - self.pred_output).sum()\n",
    "        self.pred_output_delta = final_error * self.derivative_of_sigmoid(self.pred_output)\n",
    "        \n",
    "        h_layer_error = self.pred_output_delta.dot(self.output_layer_weights.T)\n",
    "        self.hidden_layer_output_delta = h_layer_error * self.derivative_of_relu(self.output_hidden_layer)\n",
    "            \n",
    "    def update(self, inputs):\n",
    "            \n",
    "        #Updating weights and bias\n",
    "#         self.inputs = inputs\n",
    "        self.output_layer_weights = self.output_layer_weights + self.output_hidden_layer.T.dot(self.pred_output_delta) * self.alpha\n",
    "        self.output_layer_bias = self.output_layer_bias + np.sum(self.pred_output_delta,axis = 1, keepdims=True) * self.alpha\n",
    "    \n",
    "        self.hidden_layer_weights = self.hidden_layer_weights + inputs.T.dot(self.hidden_layer_output_delta) * self.alpha\n",
    "        self.hidden_layer_bias = self.hidden_layer_bias + np.sum(self.hidden_layer_output_delta, axis = 1, keepdims=True) * self.alpha\n",
    "\n",
    "        \n",
    "    def train_nn(self):\n",
    "        \n",
    "        avg_error = 0\n",
    "        for _ in range(self.epochs):\n",
    "            target_output = np.array([[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0],[1.0]]) \n",
    "            self.feedforward(self.human_feature_vectors)\n",
    "            self.backpropogation(target_output)\n",
    "            self.update(self.human_feature_vectors)\n",
    "#             if(np.absolute(avg_error - self.err)<= 0.1):\n",
    "#                 break\n",
    "            avg_error += self.err\n",
    "#             if(avg_error <= 0.1):\n",
    "#                 break\n",
    "        print('Predicted output after training Negative Images:\\n', self.pred_output)\n",
    "        print('Average Error:\\n', avg_error/len(self.human_feature_vectors))\n",
    "        \n",
    "        #save anything from training positive here         \n",
    "        \n",
    "        # reset values for negative features\n",
    "\n",
    "#         #assigning random weights\n",
    "#         self.hidden_layer_weights = np.random.uniform(-0.5, 0.5, size = (self.input_layer_neurons,self.hidden_layer_neurons))\n",
    "#         self.output_layer_weights = np.random.uniform(-0.5, 0.5, size = (self.hidden_layer_neurons,self.output_layer_neurons))\n",
    "\n",
    "#         #assigning random bias       \n",
    "#         self.hidden_layer_bias = np.random.uniform(-1, -1, size = (1,self.hidden_layer_neurons))\n",
    "#         self.output_layer_bias = np.random.uniform(-1, -1, size = (1,self.output_layer_neurons))\n",
    "\n",
    "#         self.pred_output = 0\n",
    "        \n",
    "# #         self.pred_output = np.full((2, 1), 0.0)\n",
    "        \n",
    "#         avg_error = 0\n",
    "#         for _ in range(self.epochs):\n",
    "#             target_output = np.full((10,1),0.0)\n",
    "#             self.feedforward(self.non_human_feature_vectors)\n",
    "#             self.backpropogation(target_output)\n",
    "#             self.update(self.non_human_feature_vectors)\n",
    "#             avg_error += self.err\n",
    "        \n",
    "#         print('Predicted output after training Negative Images:\\n', self.pred_output)\n",
    "#         print('Average Error:\\n', avg_error/len(self.non_human_feature_vectors))\n",
    "        \n",
    "# #         save anything from training negative here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop001008b.bmp\n",
      "160\n",
      "crop001028a.bmp\n",
      "160\n",
      "crop001030c.bmp\n",
      "160\n",
      "crop001045b.bmp\n",
      "160\n",
      "crop001047b.bmp\n",
      "160\n",
      "crop001063b.bmp\n",
      "160\n",
      "crop001275b.bmp\n",
      "160\n",
      "crop001672b.bmp\n",
      "160\n",
      "crop_000010b.bmp\n",
      "160\n",
      "person_and_bike_026a.bmp\n",
      "160\n",
      "Final Output Layer Weights:\n",
      " [[-0.03506396]\n",
      " [-0.00945494]\n",
      " [ 0.04355367]\n",
      " [-0.02548948]\n",
      " [-0.01523334]\n",
      " [ 0.03467115]\n",
      " [-0.00732109]\n",
      " [-0.02632876]\n",
      " [-0.04014109]\n",
      " [-0.00907045]\n",
      " [ 0.00594449]\n",
      " [-0.01356937]\n",
      " [ 0.0418284 ]\n",
      " [ 0.00450519]\n",
      " [ 0.02377216]\n",
      " [-0.03800691]\n",
      " [-0.04448621]\n",
      " [-0.03560071]\n",
      " [ 0.02966682]\n",
      " [-0.04956975]\n",
      " [ 0.03344621]\n",
      " [-0.0005789 ]\n",
      " [ 0.02011277]\n",
      " [-0.03317486]\n",
      " [ 0.0215322 ]\n",
      " [-0.03672639]\n",
      " [ 0.01887685]\n",
      " [-0.03639734]\n",
      " [ 0.00241343]\n",
      " [ 0.0313198 ]\n",
      " [ 0.03629281]\n",
      " [ 0.01606039]\n",
      " [ 0.01392107]\n",
      " [-0.0474192 ]\n",
      " [-0.02710417]\n",
      " [-0.01672782]\n",
      " [-0.01284441]\n",
      " [-0.00878597]\n",
      " [ 0.00518651]\n",
      " [-0.0209241 ]\n",
      " [ 0.05145679]\n",
      " [ 0.03150066]\n",
      " [-0.02942597]\n",
      " [-0.04492624]\n",
      " [ 0.03588561]\n",
      " [-0.00310313]\n",
      " [-0.03069692]\n",
      " [ 0.00185058]\n",
      " [-0.02042786]\n",
      " [ 0.03420516]\n",
      " [ 0.06356138]\n",
      " [ 0.0494032 ]\n",
      " [-0.02695733]\n",
      " [ 0.03314016]\n",
      " [-0.00574406]\n",
      " [-0.00233184]\n",
      " [ 0.00363143]\n",
      " [ 0.02200526]\n",
      " [ 0.03703682]\n",
      " [-0.00868518]\n",
      " [-0.00739082]\n",
      " [-0.0386561 ]\n",
      " [ 0.03889628]\n",
      " [ 0.03371424]\n",
      " [-0.04822   ]\n",
      " [-0.0290951 ]\n",
      " [ 0.00260417]\n",
      " [ 0.02841665]\n",
      " [-0.00925791]\n",
      " [-0.01260244]\n",
      " [ 0.02855646]\n",
      " [ 0.01086565]\n",
      " [ 0.02741908]\n",
      " [ 0.02661071]\n",
      " [ 0.04914902]\n",
      " [-0.04391088]\n",
      " [ 0.02843417]\n",
      " [ 0.01818354]\n",
      " [ 0.01411983]\n",
      " [-0.00605232]\n",
      " [ 0.01127729]\n",
      " [-0.03027456]\n",
      " [ 0.04819266]\n",
      " [-0.04645437]\n",
      " [-0.01758098]\n",
      " [-0.01646727]\n",
      " [ 0.04875544]\n",
      " [ 0.03876981]\n",
      " [ 0.02567865]\n",
      " [-0.0255762 ]\n",
      " [ 0.02932614]\n",
      " [-0.00828158]\n",
      " [-0.02903073]\n",
      " [ 0.05434636]\n",
      " [-0.04260239]\n",
      " [ 0.02434886]\n",
      " [-0.03054607]\n",
      " [ 0.01592384]\n",
      " [-0.02980155]\n",
      " [-0.01909474]\n",
      " [ 0.04931901]\n",
      " [-0.03030398]\n",
      " [ 0.03953763]\n",
      " [ 0.04296294]\n",
      " [ 0.02381799]\n",
      " [ 0.01928961]\n",
      " [ 0.01999816]\n",
      " [ 0.02730405]\n",
      " [-0.02011512]\n",
      " [ 0.00328357]\n",
      " [-0.0200818 ]\n",
      " [-0.04463991]\n",
      " [-0.0111787 ]\n",
      " [-0.03305095]\n",
      " [ 0.03609221]\n",
      " [-0.02002514]\n",
      " [ 0.04228946]\n",
      " [ 0.01596869]\n",
      " [ 0.03853159]\n",
      " [ 0.02122106]\n",
      " [ 0.01760654]\n",
      " [-0.00753969]\n",
      " [-0.03276864]\n",
      " [ 0.02458496]\n",
      " [ 0.03913029]\n",
      " [ 0.01922296]\n",
      " [-0.0304718 ]\n",
      " [ 0.01977078]\n",
      " [-0.04408645]\n",
      " [ 0.04835045]\n",
      " [ 0.03550797]\n",
      " [ 0.04528364]\n",
      " [-0.0285941 ]\n",
      " [-0.00966088]\n",
      " [ 0.00666334]\n",
      " [-0.03294294]\n",
      " [-0.00291402]\n",
      " [-0.02208338]\n",
      " [-0.0373992 ]\n",
      " [-0.03823365]\n",
      " [ 0.01122506]\n",
      " [ 0.04617786]\n",
      " [ 0.00929654]\n",
      " [-0.04050104]\n",
      " [ 0.02200411]\n",
      " [-0.03267446]\n",
      " [ 0.03684193]\n",
      " [ 0.01852832]\n",
      " [ 0.04246133]\n",
      " [ 0.00420999]\n",
      " [-0.02265442]\n",
      " [ 0.02528542]\n",
      " [-0.02797676]\n",
      " [-0.01416024]\n",
      " [ 0.04131188]\n",
      " [-0.03631439]\n",
      " [ 0.00521289]\n",
      " [ 0.01290424]\n",
      " [ 0.00311237]\n",
      " [ 0.01773054]\n",
      " [-0.04387609]\n",
      " [ 0.00612343]\n",
      " [ 0.0120406 ]\n",
      " [-0.03165428]\n",
      " [-0.04726086]\n",
      " [-0.02669009]\n",
      " [ 0.00842909]\n",
      " [-0.00539257]\n",
      " [ 0.01069959]\n",
      " [-0.02396648]\n",
      " [-0.00401782]\n",
      " [-0.02231565]\n",
      " [-0.03626503]\n",
      " [ 0.00708125]\n",
      " [ 0.04780778]\n",
      " [ 0.54941368]\n",
      " [ 0.03266924]\n",
      " [ 0.02570778]\n",
      " [-0.01464515]\n",
      " [-0.02225588]\n",
      " [ 0.03192645]\n",
      " [-0.02274053]\n",
      " [-0.01279725]\n",
      " [-0.01329858]\n",
      " [-0.02932709]\n",
      " [-0.0449676 ]\n",
      " [ 0.01398271]\n",
      " [ 0.00432972]\n",
      " [-0.01571792]\n",
      " [ 0.01930609]\n",
      " [ 0.01114678]\n",
      " [-0.0368545 ]\n",
      " [ 0.03108511]\n",
      " [-0.0497593 ]\n",
      " [-0.02309255]\n",
      " [ 0.03222512]\n",
      " [ 0.01855837]\n",
      " [-0.00979849]\n",
      " [-0.02009509]\n",
      " [-0.02284438]]\n",
      "Final Hidden Layer Weights:\n",
      " [[ 0.01840039 -0.04506006  0.03530513 ...  0.03807425  0.03647967\n",
      "   0.02532804]\n",
      " [ 0.02736452 -0.0311437   0.03036731 ... -0.0267021  -0.01851721\n",
      "   0.00045809]\n",
      " [ 0.03719464  0.01815625 -0.00928013 ... -0.03853551 -0.03533195\n",
      "  -0.02974685]\n",
      " ...\n",
      " [ 0.04108266  0.00197798 -0.01324479 ... -0.04014698  0.02525035\n",
      "  -0.01427624]\n",
      " [-0.01034481  0.00618163 -0.04495966 ... -0.01206562  0.00038902\n",
      "  -0.04054847]\n",
      " [ 0.03904189 -0.04728085 -0.02713589 ... -0.02533096  0.00214733\n",
      "  -0.00924908]]\n",
      "Final Output Layer Bias:\n",
      " [[-0.56550965]]\n",
      "Final Hidden Layer Bias:\n",
      " [[-0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685]]\n",
      "Target Output:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "After Training:\n",
      " [[0.95671755]\n",
      " [0.95761766]\n",
      " [0.91262327]\n",
      " [0.94070603]\n",
      " [0.92219206]\n",
      " [0.94065225]\n",
      " [0.93879347]\n",
      " [0.93300286]\n",
      " [0.92922528]\n",
      " [0.94921948]]\n",
      "Human detected and the predicted output is [0.95671755]\n",
      "Human detected and the predicted output is [0.95761766]\n",
      "Human detected and the predicted output is [0.91262327]\n",
      "Human detected and the predicted output is [0.94070603]\n",
      "Human detected and the predicted output is [0.92219206]\n",
      "Human detected and the predicted output is [0.94065225]\n",
      "Human detected and the predicted output is [0.93879347]\n",
      "Human detected and the predicted output is [0.93300286]\n",
      "Human detected and the predicted output is [0.92922528]\n",
      "Human detected and the predicted output is [0.94921948]\n",
      "Average error =  4.212087042934012\n",
      "crop001034b.bmp\n",
      "160\n",
      "crop001070a.bmp\n",
      "160\n",
      "crop001278a.bmp\n",
      "160\n",
      "crop001500b.bmp\n",
      "160\n",
      "person_and_bike_151a.bmp\n",
      "160\n",
      "Initial Output Layer Weights:\n",
      " [[-0.03506396]\n",
      " [-0.00945494]\n",
      " [ 0.04355367]\n",
      " [-0.02548948]\n",
      " [-0.01523334]\n",
      " [ 0.03467115]\n",
      " [-0.00732109]\n",
      " [-0.02632876]\n",
      " [-0.04014109]\n",
      " [-0.00907045]\n",
      " [ 0.00594449]\n",
      " [-0.01356937]\n",
      " [ 0.0418284 ]\n",
      " [ 0.00450519]\n",
      " [ 0.02377216]\n",
      " [-0.03800691]\n",
      " [-0.04448621]\n",
      " [-0.03560071]\n",
      " [ 0.02966682]\n",
      " [-0.04956975]\n",
      " [ 0.03344621]\n",
      " [-0.0005789 ]\n",
      " [ 0.02011277]\n",
      " [-0.03317486]\n",
      " [ 0.0215322 ]\n",
      " [-0.03672639]\n",
      " [ 0.01887685]\n",
      " [-0.03639734]\n",
      " [ 0.00241343]\n",
      " [ 0.0313198 ]\n",
      " [ 0.03629281]\n",
      " [ 0.01606039]\n",
      " [ 0.01392107]\n",
      " [-0.0474192 ]\n",
      " [-0.02710417]\n",
      " [-0.01672782]\n",
      " [-0.01284441]\n",
      " [-0.00878597]\n",
      " [ 0.00518651]\n",
      " [-0.0209241 ]\n",
      " [ 0.05145679]\n",
      " [ 0.03150066]\n",
      " [-0.02942597]\n",
      " [-0.04492624]\n",
      " [ 0.03588561]\n",
      " [-0.00310313]\n",
      " [-0.03069692]\n",
      " [ 0.00185058]\n",
      " [-0.02042786]\n",
      " [ 0.03420516]\n",
      " [ 0.06356138]\n",
      " [ 0.0494032 ]\n",
      " [-0.02695733]\n",
      " [ 0.03314016]\n",
      " [-0.00574406]\n",
      " [-0.00233184]\n",
      " [ 0.00363143]\n",
      " [ 0.02200526]\n",
      " [ 0.03703682]\n",
      " [-0.00868518]\n",
      " [-0.00739082]\n",
      " [-0.0386561 ]\n",
      " [ 0.03889628]\n",
      " [ 0.03371424]\n",
      " [-0.04822   ]\n",
      " [-0.0290951 ]\n",
      " [ 0.00260417]\n",
      " [ 0.02841665]\n",
      " [-0.00925791]\n",
      " [-0.01260244]\n",
      " [ 0.02855646]\n",
      " [ 0.01086565]\n",
      " [ 0.02741908]\n",
      " [ 0.02661071]\n",
      " [ 0.04914902]\n",
      " [-0.04391088]\n",
      " [ 0.02843417]\n",
      " [ 0.01818354]\n",
      " [ 0.01411983]\n",
      " [-0.00605232]\n",
      " [ 0.01127729]\n",
      " [-0.03027456]\n",
      " [ 0.04819266]\n",
      " [-0.04645437]\n",
      " [-0.01758098]\n",
      " [-0.01646727]\n",
      " [ 0.04875544]\n",
      " [ 0.03876981]\n",
      " [ 0.02567865]\n",
      " [-0.0255762 ]\n",
      " [ 0.02932614]\n",
      " [-0.00828158]\n",
      " [-0.02903073]\n",
      " [ 0.05434636]\n",
      " [-0.04260239]\n",
      " [ 0.02434886]\n",
      " [-0.03054607]\n",
      " [ 0.01592384]\n",
      " [-0.02980155]\n",
      " [-0.01909474]\n",
      " [ 0.04931901]\n",
      " [-0.03030398]\n",
      " [ 0.03953763]\n",
      " [ 0.04296294]\n",
      " [ 0.02381799]\n",
      " [ 0.01928961]\n",
      " [ 0.01999816]\n",
      " [ 0.02730405]\n",
      " [-0.02011512]\n",
      " [ 0.00328357]\n",
      " [-0.0200818 ]\n",
      " [-0.04463991]\n",
      " [-0.0111787 ]\n",
      " [-0.03305095]\n",
      " [ 0.03609221]\n",
      " [-0.02002514]\n",
      " [ 0.04228946]\n",
      " [ 0.01596869]\n",
      " [ 0.03853159]\n",
      " [ 0.02122106]\n",
      " [ 0.01760654]\n",
      " [-0.00753969]\n",
      " [-0.03276864]\n",
      " [ 0.02458496]\n",
      " [ 0.03913029]\n",
      " [ 0.01922296]\n",
      " [-0.0304718 ]\n",
      " [ 0.01977078]\n",
      " [-0.04408645]\n",
      " [ 0.04835045]\n",
      " [ 0.03550797]\n",
      " [ 0.04528364]\n",
      " [-0.0285941 ]\n",
      " [-0.00966088]\n",
      " [ 0.00666334]\n",
      " [-0.03294294]\n",
      " [-0.00291402]\n",
      " [-0.02208338]\n",
      " [-0.0373992 ]\n",
      " [-0.03823365]\n",
      " [ 0.01122506]\n",
      " [ 0.04617786]\n",
      " [ 0.00929654]\n",
      " [-0.04050104]\n",
      " [ 0.02200411]\n",
      " [-0.03267446]\n",
      " [ 0.03684193]\n",
      " [ 0.01852832]\n",
      " [ 0.04246133]\n",
      " [ 0.00420999]\n",
      " [-0.02265442]\n",
      " [ 0.02528542]\n",
      " [-0.02797676]\n",
      " [-0.01416024]\n",
      " [ 0.04131188]\n",
      " [-0.03631439]\n",
      " [ 0.00521289]\n",
      " [ 0.01290424]\n",
      " [ 0.00311237]\n",
      " [ 0.01773054]\n",
      " [-0.04387609]\n",
      " [ 0.00612343]\n",
      " [ 0.0120406 ]\n",
      " [-0.03165428]\n",
      " [-0.04726086]\n",
      " [-0.02669009]\n",
      " [ 0.00842909]\n",
      " [-0.00539257]\n",
      " [ 0.01069959]\n",
      " [-0.02396648]\n",
      " [-0.00401782]\n",
      " [-0.02231565]\n",
      " [-0.03626503]\n",
      " [ 0.00708125]\n",
      " [ 0.04780778]\n",
      " [ 0.54941368]\n",
      " [ 0.03266924]\n",
      " [ 0.02570778]\n",
      " [-0.01464515]\n",
      " [-0.02225588]\n",
      " [ 0.03192645]\n",
      " [-0.02274053]\n",
      " [-0.01279725]\n",
      " [-0.01329858]\n",
      " [-0.02932709]\n",
      " [-0.0449676 ]\n",
      " [ 0.01398271]\n",
      " [ 0.00432972]\n",
      " [-0.01571792]\n",
      " [ 0.01930609]\n",
      " [ 0.01114678]\n",
      " [-0.0368545 ]\n",
      " [ 0.03108511]\n",
      " [-0.0497593 ]\n",
      " [-0.02309255]\n",
      " [ 0.03222512]\n",
      " [ 0.01855837]\n",
      " [-0.00979849]\n",
      " [-0.02009509]\n",
      " [-0.02284438]]\n",
      "Initial Hidden Layer Weights:\n",
      " [[ 0.01840039 -0.04506006  0.03530513 ...  0.03807425  0.03647967\n",
      "   0.02532804]\n",
      " [ 0.02736452 -0.0311437   0.03036731 ... -0.0267021  -0.01851721\n",
      "   0.00045809]\n",
      " [ 0.03719464  0.01815625 -0.00928013 ... -0.03853551 -0.03533195\n",
      "  -0.02974685]\n",
      " ...\n",
      " [ 0.04108266  0.00197798 -0.01324479 ... -0.04014698  0.02525035\n",
      "  -0.01427624]\n",
      " [-0.01034481  0.00618163 -0.04495966 ... -0.01206562  0.00038902\n",
      "  -0.04054847]\n",
      " [ 0.03904189 -0.04728085 -0.02713589 ... -0.02533096  0.00214733\n",
      "  -0.00924908]]\n",
      "Initial Output Layer Bias:\n",
      " [[-0.56550965]]\n",
      "Initial Hidden Layer Bias:\n",
      " [[-0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685 -0.94125685\n",
      "  -0.94125685 -0.94125685]]\n",
      "Target Output:\n",
      " [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "After Training:\n",
      " [[0.91447984]\n",
      " [0.9023906 ]\n",
      " [0.92701779]\n",
      " [0.95259008]\n",
      " [0.88871981]]\n",
      "Human detected and the predicted output is [0.91447984]\n",
      "Human detected and the predicted output is [0.9023906]\n",
      "Human detected and the predicted output is [0.92701779]\n",
      "Human detected and the predicted output is [0.95259008]\n",
      "Human detected and the predicted output is [0.88871981]\n"
     ]
    }
   ],
   "source": [
    "class Driver:\n",
    "    if __name__ == '__main__':\n",
    "        \n",
    "        \n",
    "        train_path_positive = \"C://Users//anvit//OneDrive//Desktop//HoG//Training_images(Pos)\"\n",
    "        train_path_negative = \"C://Users//anvit//OneDrive//Desktop//HoG//Training_images(Neg)\"\n",
    "        test_neg = \"C://Users//anvit//OneDrive//Desktop//HoG//Test_images(Neg)\"\n",
    "        test_pos = \"C://Users//anvit//OneDrive//Desktop//HoG//Test_images(Pos)\"\n",
    "        \n",
    "        def gen_hog_vectors(path):\n",
    "            vectors = []\n",
    "            files = os.listdir(path)\n",
    "            for file in files:\n",
    "                print(file)\n",
    "                t = Preprocessing(path + '//' + file)\n",
    "                image = t.img_array\n",
    "#         print(image)\n",
    "                gray = t.gray_array\n",
    "#         print(gray)\n",
    "                g_x = t.g_x\n",
    "#         print(g_x[28][28])\n",
    "                g_y = t.g_y\n",
    "#         print(g_y[28][28])\n",
    "                g_mag = t.g_mag\n",
    "#         print(g_mag[28][28])\n",
    "                theta = t.theta\n",
    "#         print(theta[28][28])\n",
    "                h_feature_output = np.array(h_feature(g_mag,theta))\n",
    "#                 print(h_feature_output.shape)\n",
    "#                 h_feature_output = h_feature(g_mag,theta)\n",
    "                vectors.append(h_feature_output)\n",
    "                result = np.array(vectors)\n",
    "#                 print(result)\n",
    "            return result\n",
    "        \n",
    "        def train_nn(inputs, target_output):\n",
    "            \n",
    "            epochs = 10\n",
    "            alpha = 0.05\n",
    "            input_layer_neurons = 7524\n",
    "            hidden_layer_neurons = 200\n",
    "            output_layer_neurons = 1\n",
    "            err = 0\n",
    "            \n",
    "#             inputs = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "#             target_output = np.array([[0],[1],[1],[0]]) \n",
    "            \n",
    "            # Step 1: Initialize the weights and biases to have random values between −0.5 and 0.5:\n",
    "#             weights\n",
    "            hidden_layer_weights = np.random.uniform(-0.5,0.5, size = (input_layer_neurons,hidden_layer_neurons)) * 2 * alpha\n",
    "            output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons)) * 2 * alpha\n",
    "\n",
    "            # bias:\n",
    "            hidden_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,hidden_layer_neurons))\n",
    "            output_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,output_layer_neurons))\n",
    "\n",
    "#             print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "#             print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "#             print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "#             print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "                \n",
    "            # Step 2: Training the inputs\n",
    "\n",
    "            # Step 2a:\n",
    "            # Setting the corresponding activation(a1) for input_layer\n",
    "            def sigmoid(x):\n",
    "                return 1/(1 + np.exp(-x))\n",
    "\n",
    "            # We need the derivative of this for further calculations\n",
    "            def derivative_of_sigmoid(x):\n",
    "            #     derivative of sigmoid in terms of itslef is:\n",
    "                return x * (1 - x)\n",
    "            \n",
    "            def relu(x):\n",
    "                y = x.copy()\n",
    "                y[x<0] = 0\n",
    "                return y\n",
    "        \n",
    "            # Derivative of ReLU in terms of itself is:    \n",
    "            def derivative_of_relu(x):\n",
    "                y = x.copy()\n",
    "                y[x>0] = 1\n",
    "                y[x<=0] = 0\n",
    "                return y\n",
    "\n",
    "            for i in range(epochs):\n",
    "                #     Step 2b:\n",
    "                #     Feedforward\n",
    "                h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "                h_layer_a = h_layer_a + hidden_layer_bias\n",
    "                output_hidden_layer = relu(h_layer_a)\n",
    "    \n",
    "                o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "                o_layer_a = o_layer_a + output_layer_bias\n",
    "                pred_output = sigmoid(o_layer_a)\n",
    "\n",
    "                #     Step 2c:\n",
    "                #     Output error\n",
    "                \n",
    "                final_error = target_output - pred_output\n",
    "                err = err + np.absolute(target_output - pred_output).sum()\n",
    "                pred_output_delta = final_error * derivative_of_sigmoid(pred_output)\n",
    "\n",
    "                #     Step 2d:\n",
    "                #     Backpropogating the error\n",
    "                h_layer_error = pred_output_delta.dot(output_layer_weights.T)\n",
    "                hidden_layer_output_delta = h_layer_error * derivative_of_relu(output_hidden_layer)\n",
    "\n",
    "                #     Step 2e:\n",
    "                #     Updating weights and bias\n",
    "                output_layer_weights = output_layer_weights + output_hidden_layer.T.dot(pred_output_delta) * alpha\n",
    "                output_layer_bias = output_layer_bias + np.sum(pred_output_delta) * alpha\n",
    "    \n",
    "                hidden_layer_weights = hidden_layer_weights + inputs.T.dot(hidden_layer_output_delta) * alpha\n",
    "                hidden_layer_bias = hidden_layer_bias + np.sum(hidden_layer_output_delta) * alpha\n",
    "\n",
    "            print('Final Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Final Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Final Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Final Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "\n",
    "            print('Target Output:\\n', target_output)\n",
    "            print('After Training:\\n', pred_output)\n",
    "            for i in range(pred_output.shape[0]):\n",
    "                if(pred_output[i] >= 0.6):\n",
    "                    print('Human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] > 0.4 and pred_output[i] < 0.6):\n",
    "                    print('Boarderline human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i] <= 0.4):\n",
    "                    print('No human detected and the predicted output is', pred_output[i])\n",
    "            print('Average error = ', err/ (inputs.shape[0]))\n",
    "            return hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias\n",
    "                    \n",
    "        def test_nn(inputs,target_output,hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias):\n",
    "            \n",
    "            epochs = 10\n",
    "            alpha = 0.05\n",
    "            input_layer_neurons = 7524\n",
    "            hidden_layer_neurons = 200\n",
    "            output_layer_neurons = 1\n",
    "            err = 0\n",
    "            \n",
    "#             inputs = np.array([[0,0],[0,1],[1,0], [1,1]])\n",
    "#             target_output = np.array([[0],[1],[1],[0]]) \n",
    "            \n",
    "            # Step 1: Initialize the weights and biases to have random values between −0.5 and 0.5:\n",
    "            # weights\n",
    "#             hidden_layer_weights = np.random.uniform(-0.5,0.5, size = (input_layer_neurons,hidden_layer_neurons)) * 2 * 0.01\n",
    "#             output_layer_weights = np.random.uniform(-0.5,0.5, size = (hidden_layer_neurons,output_layer_neurons)) * 2 * 0.01\n",
    "\n",
    "#             # bias:\n",
    "#             hidden_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,hidden_layer_neurons))\n",
    "#             output_layer_bias = np.random.uniform(-1.0, -1.0,size = (1,output_layer_neurons))\n",
    "\n",
    "            print('Initial Output Layer Weights:\\n', output_layer_weights)\n",
    "            print('Initial Hidden Layer Weights:\\n', hidden_layer_weights)\n",
    "            print('Initial Output Layer Bias:\\n', output_layer_bias)\n",
    "            print('Initial Hidden Layer Bias:\\n', hidden_layer_bias)\n",
    "                \n",
    "            # Step 2: Training the inputs\n",
    "\n",
    "            # Step 2a:\n",
    "            # Setting the corresponding activation(a1) for input_layer\n",
    "            def sigmoid(x):\n",
    "                return 1/(1 + np.exp(-x))\n",
    "\n",
    "            # We need the derivative of this for further calculations\n",
    "            def derivative_of_sigmoid(x):\n",
    "            #     derivative of sigmoid in terms of itslef is:\n",
    "                return x * (1 - x)\n",
    "            \n",
    "            def relu(x):\n",
    "                y = x.copy()\n",
    "                y[x<0] = 0\n",
    "                return y\n",
    "        \n",
    "            # Derivative of ReLU in terms of itself is:    \n",
    "            def derivative_of_relu(x):\n",
    "                y = x.copy()\n",
    "                y[x>0] = 1\n",
    "                y[x<=0] = 0\n",
    "                return y\n",
    "\n",
    "#             for i in range(epochs):\n",
    "                #     Step 2b:\n",
    "                #     Feedforward\n",
    "            h_layer_a = np.dot(inputs, hidden_layer_weights)\n",
    "            h_layer_a = h_layer_a + hidden_layer_bias\n",
    "            output_hidden_layer = relu(h_layer_a)\n",
    "    \n",
    "            o_layer_a = np.dot(output_hidden_layer, output_layer_weights)\n",
    "            o_layer_a = o_layer_a + output_layer_bias\n",
    "            pred_output = sigmoid(o_layer_a)\n",
    "            \n",
    "            print('Target Output:\\n', target_output)\n",
    "            print('After Training:\\n', pred_output)\n",
    "            for i in range(pred_output.shape[0]):\n",
    "                if(pred_output[i][0] >= 0.6):\n",
    "                    print('Human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i][0] > 0.4 and pred_output[i][0] < 0.6):\n",
    "                    print('Boarderline human detected and the predicted output is', pred_output[i])\n",
    "                if(pred_output[i][0] <= 0.4):\n",
    "                    print('No human detected and the predicted output is', pred_output[i])\n",
    "#             print('Average error = ', err/ (inputs.shape[0]))\n",
    "        \n",
    "        positive_hog_feature_vectors = gen_hog_vectors(train_path_positive)\n",
    "        target_output_pos = np.full((10,1), 1.0, dtype = float)\n",
    "        hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias = train_nn(positive_hog_feature_vectors, target_output_pos)\n",
    "        \n",
    "        positive_test_features = gen_hog_vectors(test_pos)\n",
    "        target_output_test_pos = np.full((5,1), 1.0, dtype = float)\n",
    "        pos_test = test_nn(positive_test_features, target_output_test_pos, hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias)\n",
    "        \n",
    "#         negative_hog_feature_vectors = gen_hog_vectors(train_path_negative)\n",
    "#         target_output_neg = np.full((10,1), 0.0, dtype = float)\n",
    "#         hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias = train_nn(negative_hog_feature_vectors, target_output_neg)\n",
    "\n",
    "#         negative_test_features = gen_hog_vectors(test_neg)\n",
    "#         target_output_test_neg = np.full((5,1), 0.0, dtype = float)\n",
    "#         neg_test = test_nn(negative_test_features, target_output_test_neg, hidden_layer_weights, output_layer_weights,hidden_layer_bias,output_layer_bias)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00126032, 0.0082592 ],\n",
       "       [0.00473213, 0.00837586],\n",
       "       [0.00664615, 0.00044849]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3,2)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49141856,  0.31558122],\n",
       "       [-0.28049982, -0.41756153],\n",
       "       [ 0.34165949, -0.33732752]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " (0.5 - (-0.5))* np.random.random_sample((3, 2)) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
